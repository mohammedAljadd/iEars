{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a8e020c",
   "metadata": {},
   "source": [
    "# Multi-Face identification using CNN : Grayscale\n",
    "\n",
    "\n",
    " - In this notebook we will build our facial identification model step by step to recognize four people. Note that we have images in **jpg** format, and we will go throw the details on how to prepare the data (X, y) for our CNN model. We add an extra folder where we put images for some people that we don't know to classify them as **unknown**.\n",
    " - The model will be trained on images containing faces, what I mean by that is each image contains just the face area. We don't take the background into consideration.\n",
    " - If your images are not like that, you have two options:\n",
    "     - Manually resized them and keep just the face part.\n",
    "     - Use <a href=\"https://docs.opencv.org/3.4/db/d28/tutorial_cascade_classifier.html\">face cascade classifier</a> to extract the face part from each image and save them. If the face cascade classifier miss some faces, then you will manually resize them. \n",
    "\n",
    " - The model will run on real-time, for each video frame, we will first extract the faces using face cascade and then make predictions. For each face in an image we make prediction.\n",
    " \n",
    "First let's import some libraries :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6563aad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import random\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import load_model\n",
    "from random import randrange, uniform\n",
    "from time import time\n",
    "import glob\n",
    "import shutil\n",
    "import itertools\n",
    "from time import time\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Image size\n",
    "IMG_SIZE = 32\n",
    "\n",
    "# Dataset folder\n",
    "DATADIR = \"dataset\"\n",
    "\n",
    "# Five classes/folders\n",
    "CATEGORIES = [\"aljadd\", \"nossaiba\", \"nouhaila\", \"langze\", \"unknown\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69338739",
   "metadata": {},
   "source": [
    "# Create trainning, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a55d586d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aljadd\n",
      "nossaiba\n",
      "nouhaila\n",
      "langze\n",
      "unknown\n"
     ]
    }
   ],
   "source": [
    "for category in CATEGORIES:\n",
    "        print(category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "41d6d54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aljadd\n",
      "nossaiba\n",
      "nouhaila\n",
      "langze\n",
      "unknown\n"
     ]
    }
   ],
   "source": [
    "os.chdir('dataset')\n",
    "if os.path.isdir('train/aljadd') is False:\n",
    "    for category in CATEGORIES:\n",
    "        # Make train/category\n",
    "        os.makedirs(f'train/{category}')\n",
    "        \n",
    "        # Make test/category\n",
    "        os.makedirs(f'test/{category}')\n",
    "        \n",
    "        # Make valid/category\n",
    "        os.makedirs(f'valid/{category}')\n",
    "\n",
    "        \n",
    "    for category in CATEGORIES:\n",
    "        print(category)\n",
    "        \n",
    "        # Randomly take 400 images for training\n",
    "        for c in random.sample(glob.glob(f'{category}/{category}*'), 650):\n",
    "            shutil.move(c, f'train/{category}')\n",
    "\n",
    "        # Randomly take 70 images for validation\n",
    "        for c in random.sample(glob.glob(f'{category}/{category}*'), 70):\n",
    "            shutil.move(c, f'valid/{category}')\n",
    "\n",
    "        # Move the rest to test folder\n",
    "        for img in os.listdir(f'{category}/'):\n",
    "            image = f'{category}/{img}'\n",
    "            shutil.move(image, f'test/{category}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ff7512",
   "metadata": {},
   "source": [
    "**Move back images if necessary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8edc54f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Were are still in dataset folder\n",
    "os.chdir('dataset')\n",
    "def reset(): # Move every images back to initial folder\n",
    "    three_folders = [\"train\", \"test\", \"valid\"]\n",
    "    for category in CATEGORIES:\n",
    "        for folder in three_folders:\n",
    "            for img in os.listdir(f'{folder}/{category}'):\n",
    "                image = fr'{folder}/{category}/{img}'\n",
    "                shutil.move(image, f'{category}')\n",
    "reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2363a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cygdrive/c/Users/install.PO-ETU007/Desktop/MyProjects/iEars/Model training/Face identification/dataset\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0098ddfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bc26cb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cygdrive/c/Users/install.PO-ETU007/Desktop/MyProjects/iEars/Model training/Face identification\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bbca4e",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456dff7f",
   "metadata": {},
   "source": [
    "## 1- Create arrays of grayscale images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dd5fb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 450/450 [00:04<00:00, 105.74it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 450/450 [00:10<00:00, 41.75it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 450/450 [00:03<00:00, 121.99it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 450/450 [00:03<00:00, 113.11it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 450/450 [00:04<00:00, 106.80it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 109.95it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:01<00:00, 32.38it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 139.28it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 123.96it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 103.90it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 200/200 [00:01<00:00, 105.45it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 247/247 [00:05<00:00, 42.15it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 97/97 [00:00<00:00, 150.70it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 215/215 [00:01<00:00, 114.31it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 200/200 [00:01<00:00, 111.41it/s]\n"
     ]
    }
   ],
   "source": [
    "train = []\n",
    "valid = []\n",
    "test = []\n",
    "sets = [\"train\", \"valid\", \"test\"]\n",
    "data = [train, valid, test]\n",
    "\n",
    "for s in range(0, 3):\n",
    "    \n",
    "    # Path to train, valid or test\n",
    "    path = os.path.join(\"dataset\", sets[s])\n",
    "    \n",
    "    for category in CATEGORIES:\n",
    "        \n",
    "        # index of the class: 0, ... 4\n",
    "        class_num = CATEGORIES.index(category)\n",
    "        \n",
    "        # Path to a category inside a set : eg. train/mohammed\n",
    "        full_path = os.path.join(path, category)\n",
    "        for img in tqdm(os.listdir(full_path)):\n",
    "            \n",
    "            img_path = os.path.join(full_path,img)\n",
    "            \n",
    "            # grayscale\n",
    "            img_gray = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            img_resized = cv2.resize(img_gray, (IMG_SIZE, IMG_SIZE))\n",
    "            \n",
    "            # Add data to the three sets\n",
    "            data[s].append([img_resized, class_num]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0055e3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(train)\n",
    "random.shuffle(valid)\n",
    "random.shuffle(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6b75f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make arrays\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "X_valid = []\n",
    "y_valid = []\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "data = [train, valid, test]\n",
    "\n",
    "# Train\n",
    "for features,label in train:\n",
    "    X_train.append(features)\n",
    "    y_train.append(label)\n",
    "X_train = np.array(X_train).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# Valid\n",
    "for features,label in valid:\n",
    "    X_valid.append(features)\n",
    "    y_valid.append(label)\n",
    "X_valid = np.array(X_valid).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "y_valid = np.array(y_valid)\n",
    "\n",
    "# Test\n",
    "for features,label in test:\n",
    "    X_test.append(features)\n",
    "    y_test.append(label)\n",
    "X_test = np.array(X_test).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82c7f611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's plot one image from each set\n",
    "\n",
    "# random indexs\n",
    "i1 = random.randrange(0, len(X_train))\n",
    "i2 = random.randrange(0, len(X_valid))\n",
    "i3 = random.randrange(0, len(X_test))\n",
    "\n",
    "w = 10\n",
    "h = 10\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "columns = 3\n",
    "rows = 1\n",
    "fig.add_subplot(rows, columns, 1)\n",
    "plt.imshow(X_train[i1], cmap=\"gray\")\n",
    "plt.title(f\"Train, {y_train[i1]}\")\n",
    "fig.add_subplot(rows, columns, 2)\n",
    "plt.imshow(X_valid[i2], cmap=\"gray\")\n",
    "plt.title(f\"Valid, {y_valid[i2]}\")\n",
    "fig.add_subplot(rows, columns, 3)\n",
    "plt.imshow(X_test[i3], cmap=\"gray\")\n",
    "plt.title(f\"Test, {y_test[i3]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254c48d1",
   "metadata": {},
   "source": [
    "## 2- One hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2c6727",
   "metadata": {},
   "source": [
    "- This is very important to understand. In binary classification the y outputs takes two values (0, 1) but, in our case we have five values because class_num vary from 0 to 4. \n",
    " - y is :\n",
    "$$y = \\begin{bmatrix}0\\\\\n",
    "1\\\\\n",
    "2\\\\\n",
    "3\\\\\n",
    "4\\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "- Our CNN model accept only one hot format which is :\n",
    "$$y_{one-hot} = \\begin{bmatrix}1 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 1 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 1 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 1 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 1\\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "- Let's convert y to one hot format be using **to_categorical**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd91e9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "y_valid = to_categorical(y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72442222",
   "metadata": {},
   "source": [
    "## 3- Data augmentation for training set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b148e79f",
   "metadata": {},
   "source": [
    "   - **Data augmentation** in a technique used to increase the amount of data by adding slightly modified copies of already existing data or newly created synthetic data from existing data.\n",
    "   - Do not add soo much noise so that the model underfit the data.\n",
    "   - We will use <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\">ImageDataGenerator</a> from tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b2fd780",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = ImageDataGenerator(\n",
    "        # Rotate images by 40°\n",
    "        rotation_range = 30,\n",
    "        width_shift_range = 1,\n",
    "        height_shift_range = 1,\n",
    "        # shearing the image \n",
    "        shear_range = 1,\n",
    "        channel_shift_range = 25,\n",
    "        brightness_range = (0.95, 1.5),\n",
    "        zoom_range = 0.1,\n",
    "        horizontal_flip = True,\n",
    "        # The area left after rotationg image will be filled with same color on image\n",
    "        fill_mode='nearest')\n",
    "\n",
    "# This is for generating cropped images\n",
    "def crop(image, padding=3):\n",
    "    p = padding\n",
    "    w, h, _ = image.shape\n",
    "    cropped_image = image[p:w-p, p:h-p]\n",
    "    cropped_image = cv2.resize(cropped_image, (IMG_SIZE, IMG_SIZE))\n",
    "    cropped_image = cropped_image.reshape(IMG_SIZE, IMG_SIZE, 1)\n",
    "    return cropped_image\n",
    "\n",
    "# This is for image blurring\n",
    "def blur(image, kernel_size=(2, 2)):\n",
    "    image_blurred = cv2.blur(image, kernel_size) \n",
    "    image_blurred = cv2.resize(image_blurred, (IMG_SIZE, IMG_SIZE))\n",
    "    image_blurred = image_blurred.reshape(IMG_SIZE, IMG_SIZE, 1)\n",
    "    return image_blurred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5b810de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display for each image the new generated image\n",
    "def show(img1, img2, img3, index):\n",
    "    w = 10\n",
    "    h = 10\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    columns = 3\n",
    "    rows = 1\n",
    "    fig.add_subplot(rows, columns, 1)\n",
    "    plt.imshow(img1, cmap=\"gray\")\n",
    "    plt.title(f'Original {index}')\n",
    "    \n",
    "    fig.add_subplot(rows, columns, 2)\n",
    "    plt.imshow(img2, cmap=\"gray\")\n",
    "    plt.title(f'Cropped {index}')\n",
    "    \n",
    "    fig.add_subplot(rows, columns, 3)\n",
    "    plt.imshow(img3, cmap=\"gray\")\n",
    "    plt.title(f'Generated {index}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ae61ab04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image number 15751 out of 18000\r"
     ]
    }
   ],
   "source": [
    "# Number of generated images for each original image:\n",
    "l = 7\n",
    "\n",
    "j = 1\n",
    "index = 0\n",
    "data_aug = []\n",
    "for img in X_train:\n",
    "    img_expanded = np.expand_dims(img,0)\n",
    "    \n",
    "    # Original image\n",
    "    data_aug.append([img, y_train[index]])\n",
    "    \n",
    "    # Cropped image\n",
    "    cropped = crop(image=img, padding=3)\n",
    "    data_aug.append([cropped, y_train[index]])\n",
    "    \n",
    "    # Image blurring\n",
    "    blurred = blur(image=img, kernel_size=(2, 2))\n",
    "    data_aug.append([blurred, y_train[index]])\n",
    "    \n",
    "    # ImageDataGenerator\n",
    "    aug_iter = gen.flow(img_expanded)\n",
    "    aug_images = [next(aug_iter)[0].astype(np.uint8) for i in range(l)]\n",
    "    \n",
    "    for image in aug_images:\n",
    "        image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "        image = image.reshape((IMG_SIZE, IMG_SIZE, 1))\n",
    "        data_aug.append([image, y_train[index]])\n",
    "        # Show original image and the new created image. This is just to see is things are going well. \n",
    "        # Uncomment the following line of code if you want.\n",
    "        # show(img, cropped, image, y_train[index])\n",
    "        j += 1\n",
    "    print(f\"Image number {j} out of {(l+1)*len(X_train)}\", end='\\r')\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac1294c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the new data to X_train\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for features,label in data_aug:\n",
    "    X_train.append(features)\n",
    "    y_train.append(label)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# Normalizing for fast computation (Backpropagation)\n",
    "\n",
    "X_train = tf.keras.utils.normalize(X_train, axis=1)\n",
    "X_valid = tf.keras.utils.normalize(X_valid, axis=1)\n",
    "X_test = tf.keras.utils.normalize(X_test, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f249c530",
   "metadata": {},
   "source": [
    "# Save data in pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa1db2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I created a folder name data.pickle to save my data in\n",
    "pickle_out = open(\"data.pickle/X_train.pickle\",\"wb\")\n",
    "pickle.dump(X_train, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"data.pickle/y_train.pickle\",\"wb\")\n",
    "pickle.dump(y_train, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"data.pickle/X_valid.pickle\",\"wb\")\n",
    "pickle.dump(X_valid, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"data.pickle/y_valid.pickle\",\"wb\")\n",
    "pickle.dump(y_valid, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"data.pickle/X_test.pickle\",\"wb\")\n",
    "pickle.dump(X_test, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"data.pickle/y_test.pickle\",\"wb\")\n",
    "pickle.dump(y_test, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88848410",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e216ac49",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"data.pickle/X_train.pickle\",\"rb\")\n",
    "X_train = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"data.pickle/y_train.pickle\",\"rb\")\n",
    "y_train = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"data.pickle/X_valid.pickle\",\"rb\")\n",
    "X_valid = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"data.pickle/y_valid.pickle\",\"rb\")\n",
    "y_valid = pickle.load(pickle_in)\n",
    "\n",
    "\n",
    "pickle_in = open(\"data.pickle/X_test.pickle\",\"rb\")\n",
    "X_test = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"data.pickle/y_test.pickle\",\"rb\")\n",
    "y_test = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "048c474e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((22500, 32, 32, 1), (22500, 5))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da37ae4",
   "metadata": {},
   "source": [
    "### Build the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cd6b7c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=3, activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=3, activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#model.add(Conv2D(16, kernel_size=3, activation='relu'))\n",
    "##model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "#model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
    "#model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "# I have five classes\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "69a72a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 30, 30, 32)        320       \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 64)        18496     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 26, 26, 32)        18464     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 26, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 21632)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 5)                 108165    \n",
      "=================================================================\n",
      "Total params: 145,445\n",
      "Trainable params: 145,445\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff062b4",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    " - Epochs are number of passes on the entire dataset. The model will pass 5 times on the dataset. In each epoch, it will takes 20 images (batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ddfabe4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "900/900 [==============================] - 104s 116ms/step - loss: 0.2260 - accuracy: 0.7513 - val_loss: 0.0917 - val_accuracy: 0.9320\n",
      "Epoch 2/5\n",
      "900/900 [==============================] - 105s 117ms/step - loss: 0.0982 - accuracy: 0.9111 - val_loss: 0.0638 - val_accuracy: 0.9360\n",
      "Epoch 3/5\n",
      "900/900 [==============================] - 107s 119ms/step - loss: 0.0696 - accuracy: 0.9391 - val_loss: 0.0554 - val_accuracy: 0.9440\n",
      "Epoch 4/5\n",
      "900/900 [==============================] - 103s 115ms/step - loss: 0.0546 - accuracy: 0.9508 - val_loss: 0.0536 - val_accuracy: 0.9480\n",
      "Epoch 5/5\n",
      "900/900 [==============================] - 101s 113ms/step - loss: 0.0471 - accuracy: 0.9595 - val_loss: 0.0404 - val_accuracy: 0.9480\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x264130f3dc8>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=25, epochs=5, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5925036c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6c49a6",
   "metadata": {},
   "source": [
    "### Evaluate the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6ea6434c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 1s 21ms/step - loss: 0.0689 - accuracy: 0.9479\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06891300529241562, 0.9478623270988464]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b1d6af6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_X(threshold):\n",
    "    sample = X_test[random.randrange(0, len(X_test))]\n",
    "    image = np.expand_dims(sample, axis=0)\n",
    "    prediction = model.predict(image)\n",
    "    index = np.argmax(prediction[0])\n",
    "    probabilty = float(format(max(prediction[0]*100), \".3f\"))\n",
    "    plt.imshow(sample)\n",
    "    title = \"\"\n",
    "    if probabilty < threshold*100:\n",
    "        title = \"Unknown p<threshold\"\n",
    "    else:\n",
    "        title = f\"{CATEGORIES[index]} {probabilty}%\"\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a115ac69",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_X(threshold=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928ff14c",
   "metadata": {},
   "source": [
    "### Save the model in h5 format\n",
    "\n",
    " - Always save !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e2233579",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save(\"model_h5_format/model.h5\")\n",
    "model.save(\"model_h5_format/cnn_big_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb9c566",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "423e5189",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = load_model(\"model_h5_format/model.h5\")\n",
    "model = load_model(\"model_h5_format/cnn_big_model.h5\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9e332f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.keras.engine.sequential.Sequential'>\n"
     ]
    }
   ],
   "source": [
    "print(type(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd32ec3b",
   "metadata": {},
   "source": [
    "## Real time test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e4a60374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction from image (jpg format ...):\n",
    "def predict_from_frame(image, threshold):\n",
    "    image = tf.keras.utils.normalize(image)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    prediction = model.predict(image)\n",
    "    index = np.argmax(prediction[0])\n",
    "    probabilty = float(format(max(prediction[0]*100), \".3f\"))\n",
    "    \n",
    "    if probabilty < threshold*100:\n",
    "        title = \"Unknown 'p<threshold'\"\n",
    "    else:\n",
    "        title = f\"{CATEGORIES[index]} {probabilty}%\"\n",
    "        \n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c60b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "last_time = time()\n",
    "face_cascade = cv2.CascadeClassifier(\"cascades/data/haarcascade_frontalface_default.xml\")\n",
    "\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=2, minNeighbors=1)\n",
    "    for (x, y, w, h) in faces:\n",
    "        roi = gray[y:y+h, x:x+w]\n",
    "    \n",
    "        resized_roi = cv2.resize(roi, (IMG_SIZE, IMG_SIZE))\n",
    "    \n",
    "        color = (255, 0, 0) \n",
    "        stroke = 2\n",
    "        end_cord_x = x + w\n",
    "        end_cord_y = y + h \n",
    "        cv2.rectangle(frame, (x, y), (end_cord_x, end_cord_y), color, stroke)\n",
    "\n",
    "        p = predict_from_frame(resized_roi, threshold=0.0)   \n",
    "        cv2.putText(frame, p , (x,y), font, 1, (0, 0, 255), 1,1)\n",
    "\n",
    "\n",
    "          \n",
    "    cv2.imshow('Video', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
