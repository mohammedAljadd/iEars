{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ba0f735",
   "metadata": {},
   "source": [
    "# FaceNet\n",
    "- FaceNet is the name of the facial recognition system that was proposed by Google Researchers in 2015 in the paper titled FaceNet: A Unified Embedding for Face Recognition and Clustering\n",
    "\n",
    "- Paper link : https://arxiv.org/abs/1503.03832"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfef89d",
   "metadata": {},
   "source": [
    "# Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "6c7bf062",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FaceNet_src.architecture import * \n",
    "import os \n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import mtcnn\n",
    "import pickle \n",
    "import numpy as np \n",
    "from sklearn.preprocessing import Normalizer\n",
    "from tensorflow.keras.models import load_model\n",
    "import random\n",
    "import glob\n",
    "import shutil\n",
    "from scipy.spatial.distance import cosine\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Five classes/folders\n",
    "CATEGORIES = [\"aljadd\", \"nossaiba\", \"nouhaila\", \"langze\"]\n",
    "\n",
    "# Faces path\n",
    "source_face_path = \"dataset/\"\n",
    "destination_face_path = \"FaceNet_src/Faces/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "298f1d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty Faces folder for new training if needed!\n",
    "folder = 'FaceNet_src/Faces/'\n",
    "for category in CATEGORIES:\n",
    "    for filename in os.listdir(folder+category):\n",
    "        file_path = os.path.join(folder+category, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print('Failed to delete %s. Reason: %s' % (file_path, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "7157c9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aljadd\n",
      "nossaiba\n",
      "nouhaila\n",
      "langze\n"
     ]
    }
   ],
   "source": [
    "# Prepare images\n",
    "number_of_faces_per_class = 4\n",
    "for category in CATEGORIES:\n",
    "    print(category)\n",
    "\n",
    "    # Randomly take 5 images for training\n",
    "    for c in random.sample(glob.glob(f'{source_face_path}{category}/{category}*'), number_of_faces_per_class):\n",
    "        shutil.copy(c, f'{destination_face_path}/{category}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "ab35f0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed functions\n",
    "def normalize(img):\n",
    "    mean, std = img.mean(), img.std()\n",
    "    return (img - mean) / std\n",
    "\n",
    "def get_face(img, box):\n",
    "    x1, y1, width, height = box\n",
    "    x1, y1 = abs(x1), abs(y1)\n",
    "    x2, y2 = x1 + width, y1 + height\n",
    "    face = img[y1:y2, x1:x2]\n",
    "    return face, (x1, y1), (x2, y2)\n",
    "\n",
    "def get_encode(face_encoder, face, size):\n",
    "    face = normalize(face)\n",
    "    face = cv2.resize(face, size)\n",
    "    encode = face_encoder.predict(np.expand_dims(face, axis=0))[0]\n",
    "    return encode\n",
    "\n",
    "\n",
    "def load_pickle(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        encoding_dict = pickle.load(f)\n",
    "    return encoding_dict\n",
    "\n",
    "def detect(img ,detector,encoder,encoding_dict, wanna_distance):\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    results = detector.detect_faces(img_rgb)\n",
    "    for res in results:\n",
    "        if res['confidence'] < confidence_t:\n",
    "            continue\n",
    "        face, pt_1, pt_2 = get_face(img_rgb, res['box'])\n",
    "        encode = get_encode(encoder, face, required_size)\n",
    "        encode = l2_normalizer.transform(encode.reshape(1, -1))[0]\n",
    "        name = 'unknown'\n",
    "\n",
    "        distance = float(\"inf\")\n",
    "        for db_name, db_encode in encoding_dict.items():\n",
    "            dist = cosine(db_encode, encode)\n",
    "            if dist < recognition_t and dist < distance:\n",
    "                name = db_name\n",
    "                distance = dist\n",
    "\n",
    "        if name == 'unknown':\n",
    "            cv2.rectangle(img, pt_1, pt_2, (0, 0, 255), 2)\n",
    "            cv2.putText(img, name, pt_1, cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 1)\n",
    "        else:\n",
    "            cv2.rectangle(img, pt_1, pt_2, (0, 255, 0), 2)\n",
    "            if wanna_distance:\n",
    "                cv2.putText(img, name + f'__{distance:.2f}', (pt_1[0], pt_1[1] - 5), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
    "                            (0, 200, 200), 2)\n",
    "            else:\n",
    "                cv2.putText(img, name, (pt_1[0], pt_1[1] - 5), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
    "                            (0, 200, 200), 2)\n",
    "                \n",
    "    return img "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "60f56088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained FaceNet\n",
    "face_data = destination_face_path\n",
    "required_shape = (160,160)\n",
    "face_encoder = InceptionResNetV2()\n",
    "path = \"FaceNet_src/facenet_keras_weights.h5\"\n",
    "face_encoder.load_weights(path)\n",
    "face_detector = mtcnn.MTCNN()\n",
    "encodes = []\n",
    "encoding_dict = dict()\n",
    "l2_normalizer = Normalizer('l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "d34b3093",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4968\\1074510133.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mimg_RGB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_BGR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mface_detector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetect_faces\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_RGB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'box'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mx1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mabs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Python399\\lib\\site-packages\\mtcnn\\mtcnn.py\u001b[0m in \u001b[0;36mdetect_faces\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    298\u001b[0m         \u001b[1;31m# We pipe here each of the stages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mstage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstages\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mtotal_boxes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpoints\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Python399\\lib\\site-packages\\mtcnn\\mtcnn.py\u001b[0m in \u001b[0;36m__stage3\u001b[1;34m(self, img, total_boxes, stage_status)\u001b[0m\n\u001b[0;32m    447\u001b[0m                              width=stage_status.width, height=stage_status.height)\n\u001b[0;32m    448\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m         \u001b[0mtempimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m48\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m48\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_boxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_boxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "\n",
    "for face_names in os.listdir(face_data):\n",
    "    person_dir = os.path.join(face_data,face_names)\n",
    "\n",
    "    for image_name in os.listdir(person_dir):\n",
    "        image_path = os.path.join(person_dir,image_name)\n",
    "\n",
    "        img_BGR = cv2.imread(image_path)\n",
    "        img_RGB = cv2.cvtColor(img_BGR, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        x = face_detector.detect_faces(img_RGB)\n",
    "        x1, y1, width, height = x[0]['box']\n",
    "        x1, y1 = abs(x1) , abs(y1)\n",
    "        x2, y2 = x1+width , y1+height\n",
    "        face = img_RGB[y1:y2 , x1:x2]\n",
    "        \n",
    "        face = normalize(face)\n",
    "        face = cv2.resize(face, required_shape)\n",
    "        face_d = np.expand_dims(face, axis=0)\n",
    "        encode = face_encoder.predict(face_d)[0]\n",
    "        encodes.append(encode)\n",
    "\n",
    "    if encodes:\n",
    "        encode = np.sum(encodes, axis=0 )\n",
    "        encode = l2_normalizer.transform(np.expand_dims(encode, axis=0))[0]\n",
    "        encoding_dict[face_names] = encode\n",
    "        \n",
    "confidence_t=0.99\n",
    "recognition_t=0.5\n",
    "required_size = (160,160)\n",
    "\n",
    "path = 'FaceNet_src/encodings/encodings.pkl'\n",
    "with open(path, 'wb') as file:\n",
    "    pickle.dump(encoding_dict, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7f4953",
   "metadata": {},
   "source": [
    "# Test the model in real time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d82a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "required_shape = (160,160)\n",
    "face_encoder = InceptionResNetV2()\n",
    "path_m = \"FaceNet_src/facenet_keras_weights.h5\"\n",
    "face_encoder.load_weights(path_m)\n",
    "encodings_path = 'FaceNet_src/encodings/encodings.pkl'\n",
    "face_detector = mtcnn.MTCNN()\n",
    "encoding_dict = load_pickle(encodings_path)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret,frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"CAM NOT OPEND\") \n",
    "        break\n",
    "\n",
    "    frame = detect(frame , face_detector , face_encoder , encoding_dict, wanna_distance=True)\n",
    "\n",
    "    cv2.imshow('camera', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f402e12",
   "metadata": {},
   "source": [
    "# Testing on new images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "0d445e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img ,detector,encoder,encoding_dict):\n",
    "    label = ''\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    results = detector.detect_faces(img_rgb)\n",
    "    for res in results:\n",
    "        if res['confidence'] < confidence_t:\n",
    "            continue\n",
    "        face, pt_1, pt_2 = get_face(img_rgb, res['box'])\n",
    "        encode = get_encode(encoder, face, required_size)\n",
    "        encode = l2_normalizer.transform(encode.reshape(1, -1))[0]\n",
    "        name = 'unknown'\n",
    "\n",
    "        distance = float(\"inf\")\n",
    "        for db_name, db_encode in encoding_dict.items():\n",
    "            dist = cosine(db_encode, encode)\n",
    "            if dist < recognition_t and dist < distance:\n",
    "                name = db_name\n",
    "                distance = dist\n",
    "\n",
    "        if name == 'unknown':\n",
    "            label = 4\n",
    "        else:\n",
    "            label = CATEGORIES.index(name)\n",
    "    return label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e40bfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly choose an image\n",
    "person =  random.choice(CATEGORIES)\n",
    "for sample in random.sample(glob.glob(f'{source_face_path}{person}/{person}*'), 1):\n",
    "    img = cv2.imread(sample)\n",
    "plt.imshow(img)\n",
    "# Display prediction in the title\n",
    "plt.title(CATEGORIES[predict(img , face_detector , face_encoder , encoding_dict)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a2d33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty Faces folder for new training if needed!\n",
    "folder = 'FaceNet_src/Face_test/'\n",
    "for category in CATEGORIES:\n",
    "    for filename in os.listdir(folder+category):\n",
    "        file_path = os.path.join(folder+category, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print('Failed to delete %s. Reason: %s' % (file_path, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14dd677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare images for testing\n",
    "number_of_faces_per_class_test = 5\n",
    "test_face_path =  \"FaceNet_src/Face_test/\"\n",
    "for category in CATEGORIES:\n",
    "    print(category)\n",
    "\n",
    "    # Randomly take 100 images for training\n",
    "    for c in random.sample(glob.glob(f'{source_face_path}{category}/{category}*'), number_of_faces_per_class_test):\n",
    "        shutil.copy(c, f'{test_face_path}/{category}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ceb27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the testing accuracy\n",
    "correct_prediciton = 0\n",
    "test_img_total = number_of_faces_per_class_test*5\n",
    "ind = 0\n",
    "for person in CATEGORIES:\n",
    "    person_path = test_face_path+person\n",
    "    true_label = CATEGORIES.index(person)\n",
    "    for img in tqdm(os.listdir(person_path)):\n",
    "        img_full_path = os.path.join(person_path, img)\n",
    "        image = cv2.imread(img_full_path)\n",
    "        predicted_label = predict(image , face_detector , face_encoder , encoding_dict)\n",
    "        if predicted_label == true_label:\n",
    "            correct_prediciton += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba08d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_image(person_id):\n",
    "    person = CATEGORIES[person_id]\n",
    "    sample = random.sample(glob.glob(f'{source_face_path}{person}/{person}*'), 1)[0]\n",
    "    sample = cv2.imread(sample)\n",
    "    sample = cv2.resize(sample, (160, 160))\n",
    "    return np.array(sample)\n",
    "\n",
    "def random_4x4_image():\n",
    "    randomlist = random.sample(range(0, 4), 4)\n",
    "    ligne1 = cv2.hconcat([random_image(randomlist[0]), random_image(randomlist[1])])\n",
    "    ligne2 = cv2.hconcat([random_image(randomlist[3]), random_image(randomlist[2])])\n",
    "    image_6x6 = cv2.vconcat([ligne1, ligne2])\n",
    "    image_6x6 = cv2.copyMakeBorder(image_6x6, 30, 30, 30, 30, cv2.BORDER_CONSTANT,value=[0,0,0])\n",
    "    return image_6x6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81442de",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_6x6 = random_4x4_image()\n",
    "plt.imshow(detect(image_6x6 , face_detector , face_encoder , encoding_dict))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "8ba14701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01126832 -0.01099689  0.0445382  ...  0.10265337  0.09458227\n",
      " -0.07077383]\n",
      "Dimension :(128,)\n"
     ]
    }
   ],
   "source": [
    "def get_embedding(img ,detector=face_detector,encoder=face_encoder,encoding_dict=encoding_dict):\n",
    "    label = ''\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    results = detector.detect_faces(img_rgb)\n",
    "    for res in results:\n",
    "        if res['confidence'] < confidence_t:\n",
    "            continue\n",
    "        face, pt_1, pt_2 = get_face(img_rgb, res['box'])\n",
    "        encode = get_encode(encoder, face, required_size)\n",
    "        encode = l2_normalizer.transform(encode.reshape(1, -1))[0]\n",
    "    return encode\n",
    "np.set_printoptions(threshold=12)  # On veut afficher que 12 valeurs, car le vecteur est trop long\n",
    "embedding = get_embedding(img)\n",
    "print(embedding)\n",
    "print('Dimension :' +str(embedding.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0055af5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e155519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0c5b77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
