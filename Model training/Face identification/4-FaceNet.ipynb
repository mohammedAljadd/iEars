{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ba0f735",
   "metadata": {},
   "source": [
    "# FaceNet\n",
    "- FaceNet is the name of the facial recognition system that was proposed by Google Researchers in 2015 in the paper titled FaceNet: A Unified Embedding for Face Recognition and Clustering\n",
    "\n",
    "- Paper link : https://arxiv.org/abs/1503.03832"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfef89d",
   "metadata": {},
   "source": [
    "# Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c7bf062",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FaceNet_src.architecture import * \n",
    "import os \n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import mtcnn\n",
    "import pickle \n",
    "import numpy as np \n",
    "from sklearn.preprocessing import Normalizer\n",
    "from tensorflow.keras.models import load_model\n",
    "import random\n",
    "import glob\n",
    "import shutil\n",
    "from scipy.spatial.distance import cosine\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Five classes/folders\n",
    "CATEGORIES = [\"aljadd\", \"nossaiba\", \"nouhaila\", \"langze\"]\n",
    "\n",
    "# Faces path\n",
    "source_face_path = \"dataset/\"\n",
    "destination_face_path = \"FaceNet_src/Faces/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "298f1d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty Faces folder for new training if needed!\n",
    "folder = 'FaceNet_src/Faces/'\n",
    "for category in CATEGORIES:\n",
    "    for filename in os.listdir(folder+category):\n",
    "        file_path = os.path.join(folder+category, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print('Failed to delete %s. Reason: %s' % (file_path, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7157c9d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aljadd\n",
      "nossaiba\n",
      "nouhaila\n",
      "langze\n"
     ]
    }
   ],
   "source": [
    "# Prepare images\n",
    "number_of_faces_per_class = 20\n",
    "for category in CATEGORIES:\n",
    "    print(category)\n",
    "\n",
    "    # Randomly take 5 images for training\n",
    "    for c in random.sample(glob.glob(f'{source_face_path}{category}/{category}*'), number_of_faces_per_class):\n",
    "        shutil.copy(c, f'{destination_face_path}/{category}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab35f0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Needed functions\n",
    "def normalize(img):\n",
    "    mean, std = img.mean(), img.std()\n",
    "    return (img - mean) / std\n",
    "\n",
    "def get_face(img, box):\n",
    "    x1, y1, width, height = box\n",
    "    x1, y1 = abs(x1), abs(y1)\n",
    "    x2, y2 = x1 + width, y1 + height\n",
    "    face = img[y1:y2, x1:x2]\n",
    "    return face, (x1, y1), (x2, y2)\n",
    "\n",
    "def get_encode(face_encoder, face, size):\n",
    "    face = normalize(face)\n",
    "    face = cv2.resize(face, size)\n",
    "    encode = face_encoder.predict(np.expand_dims(face, axis=0))[0]\n",
    "    return encode\n",
    "\n",
    "\n",
    "def load_pickle(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        encoding_dict = pickle.load(f)\n",
    "    return encoding_dict\n",
    "\n",
    "def detect(img ,detector,encoder,encoding_dict, wanna_distance):\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    results = detector.detect_faces(img_rgb)\n",
    "    for res in results:\n",
    "        if res['confidence'] < confidence_t:\n",
    "            continue\n",
    "        face, pt_1, pt_2 = get_face(img_rgb, res['box'])\n",
    "        encode = get_encode(encoder, face, required_size)\n",
    "        encode = l2_normalizer.transform(encode.reshape(1, -1))[0]\n",
    "        name = 'unknown'\n",
    "\n",
    "        distance = float(\"inf\")\n",
    "        for db_name, db_encode in encoding_dict.items():\n",
    "            dist = cosine(db_encode, encode)\n",
    "            if dist < recognition_t and dist < distance:\n",
    "                name = db_name\n",
    "                distance = dist\n",
    "\n",
    "        if name == 'unknown':\n",
    "            cv2.rectangle(img, pt_1, pt_2, (0, 0, 255), 2)\n",
    "            cv2.putText(img, name, pt_1, cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 1)\n",
    "        else:\n",
    "            cv2.rectangle(img, pt_1, pt_2, (0, 255, 0), 2)\n",
    "            if wanna_distance:\n",
    "                cv2.putText(img, name + f'__{distance:.2f}', (pt_1[0], pt_1[1] - 5), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
    "                            (0, 200, 200), 2)\n",
    "            else:\n",
    "                cv2.putText(img, name, (pt_1[0], pt_1[1] - 5), cv2.FONT_HERSHEY_SIMPLEX, 1,\n",
    "                            (0, 200, 200), 2)\n",
    "                \n",
    "    return img "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60f56088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained FaceNet\n",
    "face_data = destination_face_path\n",
    "required_shape = (160,160)\n",
    "face_encoder = InceptionResNetV2()\n",
    "path = \"FaceNet_src/facenet_keras_weights.h5\"\n",
    "face_encoder.load_weights(path)\n",
    "face_detector = mtcnn.MTCNN()\n",
    "encodes = []\n",
    "encoding_dict = dict()\n",
    "l2_normalizer = Normalizer('l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d34b3093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 327 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002973AC5BCA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 328 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002973B86F430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "\n",
    "for face_names in os.listdir(face_data):\n",
    "    person_dir = os.path.join(face_data,face_names)\n",
    "\n",
    "    for image_name in os.listdir(person_dir):\n",
    "        image_path = os.path.join(person_dir,image_name)\n",
    "\n",
    "        img_BGR = cv2.imread(image_path)\n",
    "        img_RGB = cv2.cvtColor(img_BGR, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        x = face_detector.detect_faces(img_RGB)\n",
    "        x1, y1, width, height = x[0]['box']\n",
    "        x1, y1 = abs(x1) , abs(y1)\n",
    "        x2, y2 = x1+width , y1+height\n",
    "        face = img_RGB[y1:y2 , x1:x2]\n",
    "        \n",
    "        face = normalize(face)\n",
    "        face = cv2.resize(face, required_shape)\n",
    "        face_d = np.expand_dims(face, axis=0)\n",
    "        encode = face_encoder.predict(face_d)[0]\n",
    "        encodes.append(encode)\n",
    "\n",
    "    if encodes:\n",
    "        encode = np.sum(encodes, axis=0 )\n",
    "        encode = l2_normalizer.transform(np.expand_dims(encode, axis=0))[0]\n",
    "        encoding_dict[face_names] = encode\n",
    "        \n",
    "confidence_t=0.99\n",
    "recognition_t=0.5\n",
    "required_size = (160,160)\n",
    "\n",
    "path = 'FaceNet_src/encodings/encodings.pkl'\n",
    "with open(path, 'wb') as file:\n",
    "    pickle.dump(encoding_dict, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7f4953",
   "metadata": {},
   "source": [
    "# Test the model in real time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "94d82a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "required_shape = (160,160)\n",
    "face_encoder = InceptionResNetV2()\n",
    "path_m = \"FaceNet_src/facenet_keras_weights.h5\"\n",
    "face_encoder.load_weights(path_m)\n",
    "encodings_path = 'FaceNet_src/encodings/encodings.pkl'\n",
    "face_detector = mtcnn.MTCNN()\n",
    "encoding_dict = load_pickle(encodings_path)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret,frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"CAM NOT OPEND\") \n",
    "        break\n",
    "\n",
    "    frame = detect(frame , face_detector , face_encoder , encoding_dict, wanna_distance=True)\n",
    "\n",
    "    cv2.imshow('camera', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "        \n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f402e12",
   "metadata": {},
   "source": [
    "# Testing on new images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d445e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(img ,detector,encoder,encoding_dict):\n",
    "    label = ''\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    results = detector.detect_faces(img_rgb)\n",
    "    for res in results:\n",
    "        if res['confidence'] < confidence_t:\n",
    "            continue\n",
    "        face, pt_1, pt_2 = get_face(img_rgb, res['box'])\n",
    "        encode = get_encode(encoder, face, required_size)\n",
    "        encode = l2_normalizer.transform(encode.reshape(1, -1))[0]\n",
    "        name = 'unknown'\n",
    "\n",
    "        distance = float(\"inf\")\n",
    "        for db_name, db_encode in encoding_dict.items():\n",
    "            dist = cosine(db_encode, encode)\n",
    "            if dist < recognition_t and dist < distance:\n",
    "                name = db_name\n",
    "                distance = dist\n",
    "\n",
    "        if name == 'unknown':\n",
    "            label = 4\n",
    "        else:\n",
    "            label = CATEGORIES.index(name)\n",
    "    return label "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e40bfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly choose an image\n",
    "person =  random.choice(CATEGORIES)\n",
    "for sample in random.sample(glob.glob(f'{source_face_path}{person}/{person}*'), 1):\n",
    "    img = cv2.imread(sample)\n",
    "plt.imshow(img)\n",
    "# Display prediction in the title\n",
    "plt.title(CATEGORIES[predict(img , face_detector , face_encoder , encoding_dict)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a2d33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty Faces folder for new training if needed!\n",
    "folder = 'FaceNet_src/Face_test/'\n",
    "for category in CATEGORIES:\n",
    "    for filename in os.listdir(folder+category):\n",
    "        file_path = os.path.join(folder+category, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print('Failed to delete %s. Reason: %s' % (file_path, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14dd677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare images for testing\n",
    "number_of_faces_per_class_test = 5\n",
    "test_face_path =  \"FaceNet_src/Face_test/\"\n",
    "for category in CATEGORIES:\n",
    "    print(category)\n",
    "\n",
    "    # Randomly take 100 images for training\n",
    "    for c in random.sample(glob.glob(f'{source_face_path}{category}/{category}*'), number_of_faces_per_class_test):\n",
    "        shutil.copy(c, f'{test_face_path}/{category}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ceb27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the testing accuracy\n",
    "correct_prediciton = 0\n",
    "test_img_total = number_of_faces_per_class_test*5\n",
    "ind = 0\n",
    "for person in CATEGORIES:\n",
    "    person_path = test_face_path+person\n",
    "    true_label = CATEGORIES.index(person)\n",
    "    for img in tqdm(os.listdir(person_path)):\n",
    "        img_full_path = os.path.join(person_path, img)\n",
    "        image = cv2.imread(img_full_path)\n",
    "        predicted_label = predict(image , face_detector , face_encoder , encoding_dict)\n",
    "        if predicted_label == true_label:\n",
    "            correct_prediciton += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba08d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_image(person_id):\n",
    "    person = CATEGORIES[person_id]\n",
    "    sample = random.sample(glob.glob(f'{source_face_path}{person}/{person}*'), 1)[0]\n",
    "    sample = cv2.imread(sample)\n",
    "    sample = cv2.resize(sample, (160, 160))\n",
    "    return np.array(sample)\n",
    "\n",
    "def random_4x4_image():\n",
    "    randomlist = random.sample(range(0, 4), 4)\n",
    "    ligne1 = cv2.hconcat([random_image(randomlist[0]), random_image(randomlist[1])])\n",
    "    ligne2 = cv2.hconcat([random_image(randomlist[3]), random_image(randomlist[2])])\n",
    "    image_6x6 = cv2.vconcat([ligne1, ligne2])\n",
    "    image_6x6 = cv2.copyMakeBorder(image_6x6, 30, 30, 30, 30, cv2.BORDER_CONSTANT,value=[0,0,0])\n",
    "    return image_6x6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81442de",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_6x6 = random_4x4_image()\n",
    "plt.imshow(detect(image_6x6 , face_detector , face_encoder , encoding_dict))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "8ba14701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01126832 -0.01099689  0.0445382  ...  0.10265337  0.09458227\n",
      " -0.07077383]\n",
      "Dimension :(128,)\n"
     ]
    }
   ],
   "source": [
    "def get_embedding(img ,detector=face_detector,encoder=face_encoder,encoding_dict=encoding_dict):\n",
    "    label = ''\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    results = detector.detect_faces(img_rgb)\n",
    "    for res in results:\n",
    "        if res['confidence'] < confidence_t:\n",
    "            continue\n",
    "        face, pt_1, pt_2 = get_face(img_rgb, res['box'])\n",
    "        encode = get_encode(encoder, face, required_size)\n",
    "        encode = l2_normalizer.transform(encode.reshape(1, -1))[0]\n",
    "    return encode\n",
    "np.set_printoptions(threshold=12)  # On veut afficher que 12 valeurs, car le vecteur est trop long\n",
    "embedding = get_embedding(img)\n",
    "print(embedding)\n",
    "print('Dimension :' +str(embedding.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ef7ec3",
   "metadata": {},
   "source": [
    "# Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad7ad48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_face(img ,detector,encoder,encoding_dict, wanna_distance):\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    results = detector.detect_faces(img_rgb)\n",
    "    for res in results:\n",
    "        if res['confidence'] < confidence_t:\n",
    "            continue\n",
    "        face, pt_1, pt_2 = get_face(img_rgb, res['box'])\n",
    "    return face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66b4f874",
   "metadata": {},
   "outputs": [],
   "source": [
    "person = 'aljadd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0055af5",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'face' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15284\\4247596448.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mface\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_face\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mface_detector\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mface_encoder\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mencoding_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwanna_distance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'camera'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15284\\3447967403.py\u001b[0m in \u001b[0;36mextract_face\u001b[1;34m(img, detector, encoder, encoding_dict, wanna_distance)\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mface\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpt_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpt_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_face\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_rgb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'box'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mface\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'face' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# Generate dataset\n",
    "confidence_t=0.99\n",
    "required_shape = (160,160)\n",
    "face_encoder = InceptionResNetV2()\n",
    "path_m = \"FaceNet_src/facenet_keras_weights.h5\"\n",
    "face_encoder.load_weights(path_m)\n",
    "encodings_path = 'FaceNet_src/encodings/encodings.pkl'\n",
    "face_detector = mtcnn.MTCNN()\n",
    "encoding_dict = load_pickle(encodings_path)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "o = 0\n",
    "while cap.isOpened():\n",
    "    ret,frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        print(\"CAM NOT OPEND\") \n",
    "        break\n",
    "\n",
    "    face = extract_face(frame , face_detector , face_encoder , encoding_dict, wanna_distance=True)\n",
    "\n",
    "    cv2.imshow('camera', frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "    if cv2.waitKey(1) & 0xFF == ord('t'):\n",
    "        cv2.imwrite(f'sd/{person}/face{o}.jpg', cv2.cvtColor(face, cv2.COLOR_BGR2RGB))  \n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e155519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0c5b77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
