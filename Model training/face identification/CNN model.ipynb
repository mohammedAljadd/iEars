{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a8e020c",
   "metadata": {},
   "source": [
    "# Multi-Face identification using CNN : Grayscale\n",
    "\n",
    "\n",
    " - In this notebook we will build our facial identification model step by step to recognize four people. Note that we have images in **jpg** format, and we will go throw the details on how to prepare the data (X, y) for our CNN model. We add an extra folder where we put images for some people that we don't know to classify them as **unknown**.\n",
    " - The model will be trained on images containing faces, what I mean by that is each image contains just the face area. We don't take the background into consideration.\n",
    " - If your images are not like that, you have two options:\n",
    "     - Manually resized them and keep just the face part.\n",
    "     - Use <a href=\"https://docs.opencv.org/3.4/db/d28/tutorial_cascade_classifier.html\">face cascade classifier</a> to extract the face part from each image and save them. If the face cascade classifier miss some faces, then you will manually resize them. \n",
    "\n",
    " - The model will run on real-time, for each video frame, we will first extract the faces using face cascade and then make predictions. For each face in an image we make prediction.\n",
    " \n",
    "First let's import some libraries :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6563aad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import random\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import load_model\n",
    "from random import randrange, uniform\n",
    "from time import time\n",
    "import glob\n",
    "import shutil\n",
    "import itertools\n",
    "from time import time\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Image size\n",
    "IMG_SIZE = 32\n",
    "\n",
    "# Dataset folder\n",
    "DATADIR = \"dataset\"\n",
    "\n",
    "# Five classes/folders\n",
    "CATEGORIES = [\"aljadd\", \"nossaiba\", \"nouhaila\", \"langze\", \"unknown\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69338739",
   "metadata": {},
   "source": [
    "# Create trainning, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41d6d54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aljadd\n",
      "nossaiba\n",
      "nouhaila\n",
      "langze\n",
      "unknown\n"
     ]
    }
   ],
   "source": [
    "os.chdir('dataset')\n",
    "if os.path.isdir('train/aljadd') is False:\n",
    "    for category in CATEGORIES:\n",
    "        # Make train/category\n",
    "        os.makedirs(f'train/{category}')\n",
    "        \n",
    "        # Make test/category\n",
    "        os.makedirs(f'test/{category}')\n",
    "        \n",
    "        # Make valid/category\n",
    "        os.makedirs(f'valid/{category}')\n",
    "\n",
    "        \n",
    "    for category in CATEGORIES:\n",
    "        print(category)\n",
    "        \n",
    "        # Randomly take 400 images for training\n",
    "        for c in random.sample(glob.glob(f'{category}/{category}*'), 450):\n",
    "            shutil.move(c, f'train/{category}')\n",
    "\n",
    "        # Randomly take 70 images for validation\n",
    "        for c in random.sample(glob.glob(f'{category}/{category}*'), 50):\n",
    "            shutil.move(c, f'valid/{category}')\n",
    "\n",
    "        # Move the rest to test folder\n",
    "        for img in os.listdir(f'{category}/'):\n",
    "            image = f'{category}/{img}'\n",
    "            shutil.move(image, f'test/{category}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ff7512",
   "metadata": {},
   "source": [
    "**Move back images if necessary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8edc54f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Were are still in dataset folder\n",
    "os.chdir('dataset')\n",
    "def reset(): # Move every images back to initial folder\n",
    "    three_folders = [\"train\", \"test\", \"valid\"]\n",
    "    for category in CATEGORIES:\n",
    "        for folder in three_folders:\n",
    "            for img in os.listdir(f'{folder}/{category}'):\n",
    "                image = fr'{folder}/{category}/{img}'\n",
    "                shutil.move(image, f'{category}')\n",
    "reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2363a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cygdrive/c/Users/install.PO-ETU007/Desktop/MyProjects/iEars/Model training/Face identification/dataset\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0098ddfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc26cb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cygdrive/c/Users/install.PO-ETU007/Desktop/MyProjects/iEars/Model training/Face identification\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bbca4e",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456dff7f",
   "metadata": {},
   "source": [
    "## 1- Create arrays of grayscale images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9dd5fb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 450/450 [00:02<00:00, 171.62it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 450/450 [00:05<00:00, 86.96it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 450/450 [00:02<00:00, 217.97it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 450/450 [00:00<00:00, 855.90it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 450/450 [00:02<00:00, 185.34it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 185.49it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 73.35it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 206.48it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 831.03it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 166.70it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 200/200 [00:01<00:00, 170.21it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 247/247 [00:02<00:00, 93.29it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 97/97 [00:00<00:00, 229.47it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████| 215/215 [00:00<00:00, 1046.97it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 200/200 [00:01<00:00, 186.09it/s]\n"
     ]
    }
   ],
   "source": [
    "train = []\n",
    "valid = []\n",
    "test = []\n",
    "sets = [\"train\", \"valid\", \"test\"]\n",
    "data = [train, valid, test]\n",
    "\n",
    "for s in range(0, 3):\n",
    "    \n",
    "    # Path to train, valid or test\n",
    "    path = os.path.join(\"dataset\", sets[s])\n",
    "    \n",
    "    for category in CATEGORIES:\n",
    "        \n",
    "        # index of the class: 0, ... 4\n",
    "        class_num = CATEGORIES.index(category)\n",
    "        \n",
    "        # Path to a category inside a set : eg. train/mohammed\n",
    "        full_path = os.path.join(path, category)\n",
    "        for img in tqdm(os.listdir(full_path)):\n",
    "            \n",
    "            img_path = os.path.join(full_path,img)\n",
    "            \n",
    "            # grayscale\n",
    "            img_gray = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            img_resized = cv2.resize(img_gray, (IMG_SIZE, IMG_SIZE))\n",
    "            \n",
    "            # Add data to the three sets\n",
    "            data[s].append([img_resized, class_num]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0055e3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(train)\n",
    "random.shuffle(valid)\n",
    "random.shuffle(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f6b75f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make arrays\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "X_valid = []\n",
    "y_valid = []\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "data = [train, valid, test]\n",
    "\n",
    "# Train\n",
    "for features,label in train:\n",
    "    X_train.append(features)\n",
    "    y_train.append(label)\n",
    "X_train = np.array(X_train).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# Valid\n",
    "for features,label in valid:\n",
    "    X_valid.append(features)\n",
    "    y_valid.append(label)\n",
    "X_valid = np.array(X_valid).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "y_valid = np.array(y_valid)\n",
    "\n",
    "# Test\n",
    "for features,label in test:\n",
    "    X_test.append(features)\n",
    "    y_test.append(label)\n",
    "X_test = np.array(X_test).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c7f611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot one image from each set\n",
    "\n",
    "# random indexs\n",
    "i1 = random.randrange(0, len(X_train))\n",
    "i2 = random.randrange(0, len(X_valid))\n",
    "i3 = random.randrange(0, len(X_test))\n",
    "\n",
    "w = 10\n",
    "h = 10\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "columns = 3\n",
    "rows = 1\n",
    "fig.add_subplot(rows, columns, 1)\n",
    "plt.imshow(X_train[i1], cmap=\"gray\")\n",
    "plt.title(f\"Train, {y_train[i1]}\")\n",
    "fig.add_subplot(rows, columns, 2)\n",
    "plt.imshow(X_valid[i2], cmap=\"gray\")\n",
    "plt.title(f\"Valid, {y_valid[i2]}\")\n",
    "fig.add_subplot(rows, columns, 3)\n",
    "plt.imshow(X_test[i3], cmap=\"gray\")\n",
    "plt.title(f\"Test, {y_test[i3]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254c48d1",
   "metadata": {},
   "source": [
    "## 2- One hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2c6727",
   "metadata": {},
   "source": [
    "- This is very important to understand. In binary classification the y outputs takes two values (0, 1) but, in our case we have five values because class_num vary from 0 to 4. \n",
    " - y is :\n",
    "$$y = \\begin{bmatrix}0\\\\\n",
    "1\\\\\n",
    "2\\\\\n",
    "3\\\\\n",
    "4\\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "- Our CNN model accept only one hot format which is :\n",
    "$$y_{one-hot} = \\begin{bmatrix}1 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 1 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 1 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 1 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 1\\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "- Let's convert y to one hot format be using **to_categorical**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bd91e9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "y_valid = to_categorical(y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72442222",
   "metadata": {},
   "source": [
    "## 3- Data augmentation for training set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b148e79f",
   "metadata": {},
   "source": [
    "   - **Data augmentation** in a technique used to increase the amount of data by adding slightly modified copies of already existing data or newly created synthetic data from existing data.\n",
    "   - Do not add soo much noise so that the model underfit the data.\n",
    "   - We will use <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\">ImageDataGenerator</a> from tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b2fd780",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = ImageDataGenerator(\n",
    "        # Rotate images by 40°\n",
    "        rotation_range = 30,\n",
    "        width_shift_range = 1,\n",
    "        height_shift_range = 1,\n",
    "        # shearing the image \n",
    "        shear_range = 1,\n",
    "        channel_shift_range = 25,\n",
    "        brightness_range = (0.95, 1.45),\n",
    "        #zoom_range = 0.1,\n",
    "        horizontal_flip = True,\n",
    "        # The area left after rotationg image will be filled with same color on image\n",
    "        fill_mode='nearest')\n",
    "\n",
    "# This is for generating cropped images\n",
    "def crop(image, padding=3):\n",
    "    p = padding\n",
    "    w, h, _ = image.shape\n",
    "    cropped_image = image[p:w-p, p:h-p]\n",
    "    cropped_image = cv2.resize(cropped_image, (IMG_SIZE, IMG_SIZE))\n",
    "    cropped_image = cropped_image.reshape(IMG_SIZE, IMG_SIZE, 1)\n",
    "    return cropped_image\n",
    "\n",
    "# This is for image blurring\n",
    "def blur(image, kernel_size=(2, 2)):\n",
    "    image_blurred = cv2.blur(image, kernel_size) \n",
    "    image_blurred = cv2.resize(image_blurred, (IMG_SIZE, IMG_SIZE))\n",
    "    image_blurred = image_blurred.reshape(IMG_SIZE, IMG_SIZE, 1)\n",
    "    return image_blurred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a5b810de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display for each image the new generated image\n",
    "def show(img1, img2, img3, index):\n",
    "    w = 10\n",
    "    h = 10\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    columns = 3\n",
    "    rows = 1\n",
    "    fig.add_subplot(rows, columns, 1)\n",
    "    plt.imshow(img1, cmap=\"gray\")\n",
    "    plt.title(f'Original {index}')\n",
    "    \n",
    "    fig.add_subplot(rows, columns, 2)\n",
    "    plt.imshow(img2, cmap=\"gray\")\n",
    "    plt.title(f'Cropped {index}')\n",
    "    \n",
    "    fig.add_subplot(rows, columns, 3)\n",
    "    plt.imshow(img3, cmap=\"gray\")\n",
    "    plt.title(f'Generated {index}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ae61ab04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image number 18001 out of 20250\r"
     ]
    }
   ],
   "source": [
    "# Number of generated images for each original image:\n",
    "l = 8\n",
    "\n",
    "j = 1\n",
    "index = 0\n",
    "data_aug = []\n",
    "for img in X_train:\n",
    "    img_expanded = np.expand_dims(img,0)\n",
    "    \n",
    "    # Original image\n",
    "    data_aug.append([img, y_train[index]])\n",
    "    \n",
    "    # Cropped image\n",
    "    cropped = crop(image=img, padding=3)\n",
    "    data_aug.append([cropped, y_train[index]])\n",
    "    \n",
    "    # Image blurring\n",
    "    blurred = blur(image=img, kernel_size=(2, 2))\n",
    "    data_aug.append([blurred, y_train[index]])\n",
    "    \n",
    "    # ImageDataGenerator\n",
    "    aug_iter = gen.flow(img_expanded)\n",
    "    aug_images = [next(aug_iter)[0].astype(np.uint8) for i in range(l)]\n",
    "    \n",
    "    for image in aug_images:\n",
    "        image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "        image = image.reshape((IMG_SIZE, IMG_SIZE, 1))\n",
    "        data_aug.append([image, y_train[index]])\n",
    "        # Show original image and the new created image. This is just to see is things are going well. \n",
    "        # Uncomment the following line of code if you want.\n",
    "        # show(img, cropped, image, y_train[index])\n",
    "        j += 1\n",
    "    print(f\"Image number {j} out of {(l+1)*len(X_train)}\", end='\\r')\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ac1294c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the new data to X_train\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for features,label in data_aug:\n",
    "    X_train.append(features)\n",
    "    y_train.append(label)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# Normalizing for fast computation (Backpropagation)\n",
    "\n",
    "X_train = tf.keras.utils.normalize(X_train, axis=1)\n",
    "X_valid = tf.keras.utils.normalize(X_valid, axis=1)\n",
    "X_test = tf.keras.utils.normalize(X_test, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f249c530",
   "metadata": {},
   "source": [
    "# Save data in pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aa1db2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I created a folder name data.pickle to save my data in\n",
    "pickle_out = open(\"data.pickle/X_train.pickle\",\"wb\")\n",
    "pickle.dump(X_train, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"data.pickle/y_train.pickle\",\"wb\")\n",
    "pickle.dump(y_train, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"data.pickle/X_valid.pickle\",\"wb\")\n",
    "pickle.dump(X_valid, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"data.pickle/y_valid.pickle\",\"wb\")\n",
    "pickle.dump(y_valid, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"data.pickle/X_test.pickle\",\"wb\")\n",
    "pickle.dump(X_test, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"data.pickle/y_test.pickle\",\"wb\")\n",
    "pickle.dump(y_test, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88848410",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e216ac49",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"data.pickle/X_train.pickle\",\"rb\")\n",
    "X_train = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"data.pickle/y_train.pickle\",\"rb\")\n",
    "y_train = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"data.pickle/X_valid.pickle\",\"rb\")\n",
    "X_valid = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"data.pickle/y_valid.pickle\",\"rb\")\n",
    "y_valid = pickle.load(pickle_in)\n",
    "\n",
    "\n",
    "pickle_in = open(\"data.pickle/X_test.pickle\",\"rb\")\n",
    "X_test = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"data.pickle/y_test.pickle\",\"rb\")\n",
    "y_test = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "048c474e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((24750, 32, 32, 1), (24750, 5))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da37ae4",
   "metadata": {},
   "source": [
    "### Build the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cd6b7c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(28, kernel_size=3, activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "#model.add(Conv2D(16, kernel_size=3, activation='relu'))\n",
    "##model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "#model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
    "#model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "# I have five classes\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "69a72a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_6 (Conv2D)           (None, 30, 30, 28)        280       \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 30, 30, 28)        0         \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 25200)             0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 5)                 126005    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 126,285\n",
      "Trainable params: 126,285\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff062b4",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    " - Epochs are number of passes on the entire dataset. The model will pass 5 times on the dataset. In each epoch, it will takes 20 images (batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6ddfabe4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "990/990 [==============================] - 11s 11ms/step - loss: 0.4563 - accuracy: 0.3217 - val_loss: 0.2104 - val_accuracy: 0.8120\n",
      "Epoch 2/5\n",
      "990/990 [==============================] - 11s 11ms/step - loss: 0.4169 - accuracy: 0.3785 - val_loss: 0.1516 - val_accuracy: 0.8680\n",
      "Epoch 3/5\n",
      "990/990 [==============================] - 11s 11ms/step - loss: 0.4041 - accuracy: 0.3917 - val_loss: 0.1358 - val_accuracy: 0.8720\n",
      "Epoch 4/5\n",
      "990/990 [==============================] - 12s 12ms/step - loss: 0.3959 - accuracy: 0.3955 - val_loss: 0.1122 - val_accuracy: 0.9080\n",
      "Epoch 5/5\n",
      "990/990 [==============================] - 11s 11ms/step - loss: 0.3906 - accuracy: 0.4056 - val_loss: 0.1054 - val_accuracy: 0.9280\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24381d9f490>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=25, epochs=5, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6c49a6",
   "metadata": {},
   "source": [
    "### Evaluate the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6ea6434c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 0s 5ms/step - loss: 0.1433 - accuracy: 0.8790\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.14328111708164215, 0.8790406584739685]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b1d6af6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_X(threshold):\n",
    "    sample = X_test[random.randrange(0, len(X_test))]\n",
    "    image = np.expand_dims(sample, axis=0)\n",
    "    prediction = model.predict(image)\n",
    "    index = np.argmax(prediction[0])\n",
    "    probabilty = float(format(max(prediction[0]*100), \".3f\"))\n",
    "    plt.imshow(sample)\n",
    "    title = \"\"\n",
    "    if probabilty < threshold*100:\n",
    "        title = \"Unknown p<threshold\"\n",
    "    else:\n",
    "        title = f\"{CATEGORIES[index]} {probabilty}%\"\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a115ac69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict_from_X(threshold=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928ff14c",
   "metadata": {},
   "source": [
    "### Save the model in h5 format\n",
    "\n",
    " - Always save !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e2233579",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model_h5_format/model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb9c566",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "423e5189",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"model_h5_format/model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd32ec3b",
   "metadata": {},
   "source": [
    "## Real time test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e4a60374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction from image (jpg format ...):\n",
    "def predict_from_frame(image, threshold):\n",
    "    image = tf.keras.utils.normalize(image)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    prediction = model.predict(image)\n",
    "    index = np.argmax(prediction[0])\n",
    "    probabilty = float(format(max(prediction[0]*100), \".3f\"))\n",
    "    \n",
    "    if probabilty < threshold*100:\n",
    "        title = \"Unknown 'p<threshold'\"\n",
    "    else:\n",
    "        title = f\"{CATEGORIES[index]} {probabilty}%\"\n",
    "        \n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "51c60b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "last_time = time()\n",
    "face_cascade = cv2.CascadeClassifier(\"cascades/data/haarcascade_frontalface_default.xml\")\n",
    "\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=2, minNeighbors=1)\n",
    "    for (x, y, w, h) in faces:\n",
    "        roi = gray[y:y+h, x:x+w]\n",
    "    \n",
    "        resized_roi = cv2.resize(roi, (IMG_SIZE, IMG_SIZE))\n",
    "    \n",
    "        color = (255, 0, 0) \n",
    "        stroke = 2\n",
    "        end_cord_x = x + w\n",
    "        end_cord_y = y + h \n",
    "        cv2.rectangle(frame, (x, y), (end_cord_x, end_cord_y), color, stroke)\n",
    "\n",
    "        p = predict_from_frame(resized_roi, threshold=0.0)   \n",
    "        cv2.putText(frame, p , (x,y), font, 1, (0, 0, 255), 1,1)\n",
    "\n",
    "\n",
    "          \n",
    "    cv2.imshow('Video', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42a9de0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad4d9b48",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predict_from_frame' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\INSTAL~1.PO-\\AppData\\Local\\Temp/ipykernel_1152/1311106873.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrectangle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mend_cord_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_cord_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstroke\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m             \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_from_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresized_roi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m             \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mputText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfont\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m255\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'predict_from_frame' is not defined"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "last_time = time()\n",
    "face_cascade = cv2.CascadeClassifier(\"cascades/data/haarcascade_frontalface_default.xml\")\n",
    "\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.5, minNeighbors=1)\n",
    "    if len(faces) != 0:\n",
    "        for (x, y, w, h) in faces:\n",
    "            roi = gray[y:y+h, x:x+w]\n",
    "\n",
    "            resized_roi = cv2.resize(roi, (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "            color = (255, 0, 0) \n",
    "            stroke = 2\n",
    "            end_cord_x = x + w\n",
    "            end_cord_y = y + h \n",
    "            cv2.rectangle(frame, (x, y), (end_cord_x, end_cord_y), color, stroke)\n",
    "\n",
    "            p = predict_from_frame(resized_roi, threshold=0.0)   \n",
    "            cv2.putText(frame, p , (x,y), font, 1, (0, 0, 255), 1,1)\n",
    "\n",
    "    else:\n",
    "        cv2.putText(frame, \"No face is detected\" , (x,y), font, 1, (0, 0, 255), 1,1)\n",
    "    \n",
    "    cv2.imshow('Video', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89448bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a5ebd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(faces) != 0:\n",
    "        for (x, y, w, h) in faces:\n",
    "            roi = gray[y:y+h, x:x+w]\n",
    "\n",
    "            resized_roi = cv2.resize(roi, (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "            color = (255, 0, 0) \n",
    "            stroke = 2\n",
    "            end_cord_x = x + w\n",
    "            end_cord_y = y + h \n",
    "            cv2.rectangle(frame, (x, y), (end_cord_x, end_cord_y), color, stroke)\n",
    "\n",
    "            p = predict_from_frame(resized_roi, threshold=0.0)   \n",
    "            cv2.putText(frame, p , (x,y), font, 1, (0, 0, 255), 1,1)\n",
    "\n",
    "    else:\n",
    "        cv2.putText(frame, \"No face is detected\" , (x,y), font, 1, (0, 0, 255), 1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc0c131c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIES = [\"aljadd\", \"nossaiba\", \"nouhaila\", \"langze\", \"unknown\", \"no person\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7112320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 1, 2, 4, 3, 5, 2, 4, 5, 3, 2, 3, 3, 5, 2, 4, 5, 3, 2, 3, 5, 5]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [1, 2, 1, 2, 4, 3, 5, 2, 4, 5, 3, 2, 3, 3, 5, 2, 4, 5, 3, 2, 3, 5, 5]\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c196b399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 4, 3, 5]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_no_duplicates = []\n",
    "for i in predictions :\n",
    "    if i not in predictions_no_duplicates:\n",
    "        predictions_no_duplicates.append(i)\n",
    "predictions_no_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7991778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 3, 1, 6]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "occurences = []\n",
    "for p in predictions_no_duplicates:\n",
    "    occurence = predictions.count(p)\n",
    "    # Do not consider duplicates of faces of known people, because it's due to face cascade errors\n",
    "    if p not in [0, 1, 2, 3]:\n",
    "        occurences.append(occurence)\n",
    "    else:\n",
    "        occurences.append(1)\n",
    "occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2889a209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Predicitions : [1, 2, 4, 3, 5], Occurences : [1, 1, 3, 1, 6]'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"Predicitions : {predictions_no_duplicates}, Occurences : {occurences}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e1753a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index of \"no person is detected\"\n",
    "index_no_person = predictions_no_duplicates.index(5)\n",
    "predictions_no_duplicates.pop(index_no_person)\n",
    "occurences.pop(index_no_person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b194e49b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_no_duplicates.pop(index_no_person)\n",
    "occurences.pop(index_no_person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6cfff81b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Predicitions : [1, 2, 4, 3], Occurences : [1, 1, 3, 1]'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"Predicitions : {predictions_no_duplicates}, Occurences : {occurences}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "fa07dffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_faces = len(predictions_no_duplicates)\n",
    "predictions = predictions_no_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "614b818c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Il y a  nossaiba,  nouhaila, 3 unknown,  langze, et 6 no person.'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def replace(i):\n",
    "            if i == 1:\n",
    "                return \"\"\n",
    "            else:\n",
    "                return str(i)\n",
    "\n",
    "result = \"Il y a \"\n",
    "if number_of_faces == 1:\n",
    "    result += f\"{replace(occurences[0])}{CATEGORIES[predictions[0]]}\"\n",
    "\n",
    "elif number_of_faces == 2:\n",
    "    result += f\"{replace(occurences[0])} {CATEGORIES[predictions[0]]} et {replace(occurences[1])} {CATEGORIES[predictions[1]]}\"\n",
    "\n",
    "else:\n",
    "    i = 0\n",
    "    for p in predictions_no_duplicates:\n",
    "        if i < number_of_faces-1:\n",
    "            result += f\"{replace(occurences[i])} {CATEGORIES[p]}, \"\n",
    "        else:\n",
    "            result += f\"et {replace(occurences[i])} {CATEGORIES[p]}.\"\n",
    "        i += 1\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "50a80f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1816377",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
