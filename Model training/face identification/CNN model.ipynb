{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a8e020c",
   "metadata": {},
   "source": [
    "# Multi-Face identification using CNN\n",
    "\n",
    "\n",
    " - In this notebook we will build our facial identification model step by step to recognize four people. Note that we have images in **jpg** format, and we will go throw the details on how to prepare the data (X, y) for our CNN model. We add an extra folder where we put images for some people that we don't know to classify them as **unknown**.\n",
    " - The model will be trained on images containing faces, what I mean by that is each image contains just the face area. We don't take the background into consideration.\n",
    " - If your images are not like that, you have two options:\n",
    "     - Manually resized them and keep just the face part.\n",
    "     - Use <a href=\"https://docs.opencv.org/3.4/db/d28/tutorial_cascade_classifier.html\">face cascade classifier</a> to extract the face part from each image and save them. If the face cascade classifier miss some faces, then you will manually resize them. \n",
    "\n",
    " - The model will run on real-time, for each video frame, we will first extract the faces using face cascade and then make predictions. For each face in an image we make prediction.\n",
    " \n",
    "First let's import some libraries :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6563aad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import random\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import load_model\n",
    "from random import randrange, uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf3ecc2",
   "metadata": {},
   "source": [
    "    \n",
    "   - We will set the image shape to 32x32. It is too small because the face does not take much space in an image.\n",
    "   - DATACROP : directory where we have five folders, each one contains images for each person. The 5th folder is for the unknown people. You can dowload images from Kaggle for example.\n",
    "   - CATEGORIES : name of the five folders(people eg: Said, Blake ... and unknown for unknown people).\n",
    "   - One example: **\"../DATACROP/SAID/hello.jpg\"**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac55992",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 32\n",
    "DATACROP = \"dataset_crop\"\n",
    "CATEGORIES = [\"AL JADD\", \"Nossaiba\", \"EL NABAOUI\", \"YE\", \"unknown\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8dca09",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for category in CATEGORIES:\n",
    "    path = os.path.join(DATACROP,category)\n",
    "    class_num = CATEGORIES.index(category) \n",
    "    for img in tqdm(os.listdir(path)):\n",
    "        img_path = os.path.join(path,img)\n",
    "        img_gray = cv2.imread(img_path, 0) \n",
    "        img_resized = cv2.resize(img_gray, (IMG_SIZE, IMG_SIZE))\n",
    "        data.append([img_resized, class_num])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1144628f",
   "metadata": {},
   "source": [
    "- Let's shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f72b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92836b07",
   "metadata": {},
   "source": [
    "- Let's make an array that contains our images.\n",
    "- data contains (X, y).\n",
    "\n",
    "$$X = \\begin{bmatrix}---- x1 ----\\\\\n",
    "---- x2 ----\\\\\n",
    ".\\\\\n",
    ".\\\\\n",
    "---- xm ----\\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "- Where **m** is the total number of images. Let's take an x1 for example. The shape of x1 is **(32, 32, 1)** because we set **IMG_SIZE** = 32 and the number 1 in the 3rd component becasue it's on **grayscale**.\n",
    "\n",
    "$$y = \\begin{bmatrix}0\\\\\n",
    "1\\\\\n",
    "2\\\\\n",
    "3\\\\\n",
    "4\\\\\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee57152e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for features,label in data:\n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "y = np.array(y)\n",
    "\n",
    "\n",
    "print(f'X shape is :{X.shape} \\ny shape is :{y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a942f1",
   "metadata": {},
   "source": [
    " - **Let's split our data, remember to specify stratify=y so that the classes percentage in train set will be the same in the test set.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37460433",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844a875d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ecc716",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c354f44",
   "metadata": {},
   "source": [
    "- If you want to count the percentage of a classe on the training or the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f642669",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(Y, element):\n",
    "    i = 0\n",
    "    for y in Y:\n",
    "        y = np.array(y)\n",
    "        element = np.array(element)\n",
    "        if y == element:\n",
    "            i += 1\n",
    "        else:\n",
    "            pass\n",
    "    print(f\"Percentage of the class {element} is {np.round(100*i/len(Y))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6a0431",
   "metadata": {},
   "outputs": [],
   "source": [
    "count(y_train, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2c6727",
   "metadata": {},
   "source": [
    "### One hot for outputs\n",
    "\n",
    " - This is very important to understand. In binary classification the y outputs takes two values (0, 1) but, in our case we have five values because class_num vary from 0 to 4. \n",
    " - y is :\n",
    "$$y = \\begin{bmatrix}0\\\\\n",
    "1\\\\\n",
    "2\\\\\n",
    "3\\\\\n",
    "4\\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "- Our CNN model accept only one hot format which is :\n",
    "$$y_{one-hot} = \\begin{bmatrix}1 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 1 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 1 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 1 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 1\\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "- Let's convert y to one hot format be using **to_categorical**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bfb515",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = to_categorical(y_test)\n",
    "y_train = to_categorical(y_train)\n",
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52328fd5",
   "metadata": {},
   "source": [
    "### Data augmentation\n",
    "\n",
    "   - **Data augmentation** in a technique used to increase the amount of data by adding slightly modified copies of already existing data or newly created synthetic data from existing data.\n",
    "   - Do not add soo much noise so that the model underfit the data.\n",
    "   - We will use <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\">ImageDataGenerator</a> from tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9152c9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = ImageDataGenerator(\n",
    "        # Rotate images by 40Â°\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        # shearing the image \n",
    "        shear_range=0.1,\n",
    "        channel_shift_range = 23,\n",
    "        brightness_range=(0.9, 1.5),\n",
    "        zoom_range=0.1,\n",
    "        horizontal_flip=True,\n",
    "        # The area left after rotationg image will be filled with same color on image\n",
    "        fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061b6e8d",
   "metadata": {},
   "source": [
    "- If you want to display every image along with new created images :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d908468c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(img1, img2, index):\n",
    "    w = 10\n",
    "    h = 10\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    columns = 2\n",
    "    rows = 1\n",
    "    fig.add_subplot(rows, columns, 1)\n",
    "    plt.imshow(img1)\n",
    "    plt.title(f'Full quality {index}')\n",
    "    fig.add_subplot(rows, columns, 2)\n",
    "    plt.imshow(img2)\n",
    "    plt.title(f'Low quality {index}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25947310",
   "metadata": {},
   "source": [
    "- Set the varable **l** to any number you want. It will determines how many new images will be created for each original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac254be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try changing it:\n",
    "l = 20\n",
    "\n",
    "j = 1\n",
    "index = 0\n",
    "data_aug = []\n",
    "for img in X_train:\n",
    "    print(f\"Image number {j} out of {(l+1)*len(X_train)}\", end='\\r')\n",
    "    img_expanded = np.expand_dims(img,0)\n",
    "    aug_iter = gen.flow(img_expanded)\n",
    "    aug_images = [next(aug_iter)[0].astype(np.uint8) for i in range(l)]\n",
    "    j += 1\n",
    "    for image in aug_images:\n",
    "        s = image\n",
    "        image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "        image = image.reshape(IMG_SIZE, IMG_SIZE, 1)\n",
    "        data_aug.append([image, y_train[index]])\n",
    "        # Show original image and the new created image. This is just to see is things are going well. \n",
    "        # Uncomment the following line of code if you want.\n",
    "        # show(img, image, y_train[index])\n",
    "        j += 1\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e3a80d",
   "metadata": {},
   "source": [
    "### Shuffle the data again\n",
    "\n",
    "  - Remember to shuffle the data so that the model gets diffrent classes while training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2a8823",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(data_aug)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ac7009",
   "metadata": {},
   "source": [
    "### Save data as pickle format\n",
    "\n",
    " - Always save your data so next time you will just load X and y without repeating the above codes again "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c91b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for features,label in data_aug:\n",
    "    X_train.append(features)\n",
    "    y_train.append(label)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# Rescale for fast computation (Backpropagation)\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "\n",
    "# I created a folder name data.pickle to save my data in\n",
    "pickle_out = open(\"data.pickle/X_train.pickle\",\"wb\")\n",
    "pickle.dump(X_train, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"data.pickle/y_train.pickle\",\"wb\")\n",
    "pickle.dump(y_train, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"data.pickle/X_test.pickle\",\"wb\")\n",
    "pickle.dump(X_test, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"data.pickle/y_test.pickle\",\"wb\")\n",
    "pickle.dump(y_test, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672b236d",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0ec3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"data.pickle/X_train.pickle\",\"rb\")\n",
    "X_train = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"data.pickle/y_train.pickle\",\"rb\")\n",
    "y_train = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"data.pickle/X_test.pickle\",\"rb\")\n",
    "X_test = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"data.pickle/y_test.pickle\",\"rb\")\n",
    "y_test = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da37ae4",
   "metadata": {},
   "source": [
    "### Build the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6b7c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# The input shape is the image shape (a, b, 1) 'It is grayscale'\n",
    "model.add(Conv2D(32, kernel_size=3, activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1)))\n",
    "\n",
    "model.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
    "model.add(Dropout(0.7))\n",
    "model.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "# I have five classes, so make sure to use the right number bellow\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff062b4",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    " - Epochs are number of passes on the entire dataset. The model will pass 5 times on the dataset. In each epoch, it will takes 20 images (batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddfabe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, batch_size=20, epochs=3, validation_split=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6c49a6",
   "metadata": {},
   "source": [
    "### Evaluate the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4742b4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)\n",
    "\n",
    "# If the model suffer from overfitting or underfitting, try changing your CNN model architecture by adding Dropout layers or \n",
    "# more hidden layers... \n",
    "# You may spend a lot of time retraining your model. You have to know what your model suffer from, \n",
    "# and try to solve the problem. This is what deep learning looks like!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928ff14c",
   "metadata": {},
   "source": [
    "### Save the model in h5 format\n",
    "\n",
    " - Always save !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2233579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'll save my model in folder called model_h5_format\n",
    "model.save(\"model_h5_format/my_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb9c566",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423e5189",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = load_model(\"model_h5_format/my_model.h5\")\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcdf7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction from X_test:\n",
    "# I will set threshold = 0.5 so if the model gives for example 0.4 probability that an image belongs to a class, I will consider\n",
    "# not taking it into consideration\n",
    "def predict_from_X(i, threshold):\n",
    "    x = X_test[i]\n",
    "    plt.imshow(x)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    prediction = new_model.predict(x)\n",
    "    i  =  np.array(np.where(((prediction >= threshold).astype('int32')[0]) == 1))[0]\n",
    "    who = \"\"\n",
    "    \n",
    "    if not i.shape == (0,):\n",
    "        if i[0] == 0:\n",
    "                who = \"AL JADD\"\n",
    "\n",
    "        elif i[0] == 1:\n",
    "            who = \"Nossaiba\"\n",
    "\n",
    "        elif i[0] == 2:\n",
    "            who = \"EL NABAOUI\"\n",
    "\n",
    "        elif i[0] == 3:\n",
    "            who = \"YE Langze\"\n",
    "        else:\n",
    "            who = \"Unknown\"\n",
    "    else:\n",
    "        who = \"Unknown\"\n",
    "    print(who)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1b7cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_X(randrange(0, X_test.shape[0]-1), threshold=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd32ec3b",
   "metadata": {},
   "source": [
    "## Real time test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a60374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction from image (jpg format ...):\n",
    "def predict_from_frame(image, threshold):\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    prediction = new_model.predict(image)\n",
    "    i  =  np.array(np.where(((prediction >= threshold).astype('int32')[0]) == 1))[0]\n",
    "    who = \"\"\n",
    "    if not i.shape == (0,):\n",
    "        if i[0] == 0:\n",
    "                who = \"AL JADD\"\n",
    "\n",
    "        elif i[0] == 1:\n",
    "            who = \"Nossaiba\"\n",
    "\n",
    "        elif i[0] == 2:\n",
    "            who = \"EL NABAOUI\"\n",
    "\n",
    "        elif i[0] == 3:\n",
    "            who = \"YE Langze\"\n",
    "        else:\n",
    "            who = \"Unknown\"\n",
    "    else:\n",
    "        who = \"Unknown\"\n",
    "    return who"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c60b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "cap = cv2.VideoCapture(0)\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "last_time = time()\n",
    "face_cascade = cv2.CascadeClassifier(\"cascades/data/haarcascade_frontalface_default.xml\")\n",
    "i = 1413\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=2, minNeighbors=1)\n",
    "    for (x, y, w, h) in faces:\n",
    "            roi_gray = gray[y:y+h, x:x+w]\n",
    "            roi_sav = frame[y:y+h, x:x+w]\n",
    "            resized_roi_gray = cv2.resize(roi_gray, (IMG_SIZE, IMG_SIZE))\n",
    "            # When you wanna make prediction, you have to rescale the image first. I pass two days figuring out why my model\n",
    "            # outputing the same result. As the model was trained on rescaled images, the prediction must done on rescaled\n",
    "            # images too.\n",
    "            resized_roi_gray_recaled = resized_roi_gray/255.0\n",
    "            \n",
    "            color = (255, 0, 0) \n",
    "            stroke = 2\n",
    "            end_cord_x = x + w\n",
    "            end_cord_y = y + h \n",
    "            cv2.rectangle(frame, (x, y), (end_cord_x, end_cord_y), color, stroke)\n",
    "            \n",
    "            p = predict_from_frame(resized_roi_gray_recaled, threshold=0.8)   \n",
    "            cv2.putText(frame, p , (x,y), font, 1, (0, 0, 255), 1,1)\n",
    "           \n",
    "        \n",
    "          \n",
    "    cv2.imshow('Video', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
