{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a8e020c",
   "metadata": {},
   "source": [
    "# Multi-Face identification using CNN : Grayscale\n",
    "\n",
    "\n",
    " - In this notebook we will build our facial identification model step by step to recognize four people. Note that we have images in **jpg** format, and we will go throw the details on how to prepare the data (X, y) for our CNN model. We add an extra folder where we put images for some people that we don't know to classify them as **unknown**.\n",
    " - The model will be trained on images containing faces, what I mean by that is each image contains just the face area. We don't take the background into consideration.\n",
    " - If your images are not like that, you have two options:\n",
    "     - Manually resized them and keep just the face part.\n",
    "     - Use <a href=\"https://docs.opencv.org/3.4/db/d28/tutorial_cascade_classifier.html\">face cascade classifier</a> to extract the face part from each image and save them. If the face cascade classifier miss some faces, then you will manually resize them. \n",
    "\n",
    " - The model will run on real-time, for each video frame, we will first extract the faces using face cascade and then make predictions. For each face in an image we make prediction.\n",
    " \n",
    "First let's import some libraries :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6563aad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import random\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import load_model\n",
    "from random import randrange, uniform\n",
    "from time import time\n",
    "import glob\n",
    "import shutil\n",
    "import itertools\n",
    "from time import time\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Image size\n",
    "IMG_SIZE = 32\n",
    "\n",
    "# Dataset folder\n",
    "DATADIR = \"dataset\"\n",
    "\n",
    "# Five classes/folders\n",
    "CATEGORIES = [\"aljadd\", \"nossaiba\", \"nouhaila\", \"langze\", \"unknown\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69338739",
   "metadata": {},
   "source": [
    "# Create trainning, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41d6d54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aljadd\n",
      "nossaiba\n",
      "nouhaila\n",
      "langze\n",
      "unknown\n"
     ]
    }
   ],
   "source": [
    "os.chdir('dataset')\n",
    "if os.path.isdir('train/aljadd') is False:\n",
    "    for category in CATEGORIES:\n",
    "        # Make train/category\n",
    "        os.makedirs(f'train/{category}')\n",
    "        \n",
    "        # Make test/category\n",
    "        os.makedirs(f'test/{category}')\n",
    "        \n",
    "        # Make valid/category\n",
    "        os.makedirs(f'valid/{category}')\n",
    "\n",
    "        \n",
    "    for category in CATEGORIES:\n",
    "        print(category)\n",
    "        \n",
    "        # Randomly take 400 images for training\n",
    "        for c in random.sample(glob.glob(f'{category}/{category}*'), 500):\n",
    "            shutil.move(c, f'train/{category}')\n",
    "\n",
    "        # Randomly take 70 images for validation\n",
    "        for c in random.sample(glob.glob(f'{category}/{category}*'), 50):\n",
    "            shutil.move(c, f'valid/{category}')\n",
    "\n",
    "        # Move the rest to test folder\n",
    "        for img in os.listdir(f'{category}/'):\n",
    "            image = f'{category}/{img}'\n",
    "            shutil.move(image, f'test/{category}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ff7512",
   "metadata": {},
   "source": [
    "**Move back images if necessary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8edc54f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Were are still in dataset folder\n",
    "#os.chdir('dataset')\n",
    "def reset(): # Move every images back to initial folder\n",
    "    three_folders = [\"train\", \"test\", \"valid\"]\n",
    "    for category in CATEGORIES:\n",
    "        for folder in three_folders:\n",
    "            for img in os.listdir(f'{folder}/{category}'):\n",
    "                image = fr'{folder}/{category}/{img}'\n",
    "                shutil.move(image, f'{category}')\n",
    "reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2363a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cygdrive/c/Users/install.PO-ETU007/Desktop/MyProjects/iEars/Model training/Face identification/dataset\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0098ddfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc26cb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cygdrive/c/Users/install.PO-ETU007/Desktop/MyProjects/iEars/Model training/Face identification\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bbca4e",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456dff7f",
   "metadata": {},
   "source": [
    "## 1- Create arrays of grayscale images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dd5fb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 500/500 [00:02<00:00, 179.20it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:05<00:00, 88.48it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 500/500 [00:07<00:00, 64.21it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 500/500 [00:02<00:00, 198.78it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 500/500 [00:02<00:00, 177.56it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 173.08it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 95.57it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 78.12it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 176.52it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 183.42it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 150/150 [00:00<00:00, 177.06it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 197/197 [00:02<00:00, 87.07it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 53/53 [00:01<00:00, 46.35it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 166/166 [00:00<00:00, 175.93it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 150/150 [00:00<00:00, 179.21it/s]\n"
     ]
    }
   ],
   "source": [
    "train = []\n",
    "valid = []\n",
    "test = []\n",
    "sets = [\"train\", \"valid\", \"test\"]\n",
    "data = [train, valid, test]\n",
    "\n",
    "for s in range(0, 3):\n",
    "    \n",
    "    # Path to train, valid or test\n",
    "    path = os.path.join(\"dataset\", sets[s])\n",
    "    \n",
    "    for category in CATEGORIES:\n",
    "        \n",
    "        # index of the class: 0, ... 4\n",
    "        class_num = CATEGORIES.index(category)\n",
    "        \n",
    "        # Path to a category inside a set : eg. train/mohammed\n",
    "        full_path = os.path.join(path, category)\n",
    "        for img in tqdm(os.listdir(full_path)):\n",
    "            \n",
    "            img_path = os.path.join(full_path,img)\n",
    "            \n",
    "            # grayscale\n",
    "            img_gray = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            img_resized = cv2.resize(img_gray, (IMG_SIZE, IMG_SIZE))\n",
    "            \n",
    "            # Add data to the three sets\n",
    "            data[s].append([img_resized, class_num]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0055e3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(train)\n",
    "random.shuffle(valid)\n",
    "random.shuffle(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6b75f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make arrays\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "X_valid = []\n",
    "y_valid = []\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "data = [train, valid, test]\n",
    "\n",
    "# Train\n",
    "for features,label in train:\n",
    "    X_train.append(features)\n",
    "    y_train.append(label)\n",
    "X_train = np.array(X_train).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# Valid\n",
    "for features,label in valid:\n",
    "    X_valid.append(features)\n",
    "    y_valid.append(label)\n",
    "X_valid = np.array(X_valid).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "y_valid = np.array(y_valid)\n",
    "\n",
    "# Test\n",
    "for features,label in test:\n",
    "    X_test.append(features)\n",
    "    y_test.append(label)\n",
    "X_test = np.array(X_test).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82c7f611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x576 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's plot one image from each set\n",
    "\n",
    "# random indexs\n",
    "i1 = random.randrange(0, len(X_train))\n",
    "i2 = random.randrange(0, len(X_valid))\n",
    "i3 = random.randrange(0, len(X_test))\n",
    "\n",
    "w = 10\n",
    "h = 10\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "columns = 3\n",
    "rows = 1\n",
    "fig.add_subplot(rows, columns, 1)\n",
    "plt.imshow(X_train[i1], cmap=\"gray\")\n",
    "plt.title(f\"Train, {y_train[i1]}\")\n",
    "fig.add_subplot(rows, columns, 2)\n",
    "plt.imshow(X_valid[i2], cmap=\"gray\")\n",
    "plt.title(f\"Valid, {y_valid[i2]}\")\n",
    "fig.add_subplot(rows, columns, 3)\n",
    "plt.imshow(X_test[i3], cmap=\"gray\")\n",
    "plt.title(f\"Test, {y_test[i3]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254c48d1",
   "metadata": {},
   "source": [
    "## 2- One hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2c6727",
   "metadata": {},
   "source": [
    "- This is very important to understand. In binary classification the y outputs takes two values (0, 1) but, in our case we have five values because class_num vary from 0 to 4. \n",
    " - y is :\n",
    "$$y = \\begin{bmatrix}0\\\\\n",
    "1\\\\\n",
    "2\\\\\n",
    "3\\\\\n",
    "4\\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "- Our CNN model accept only one hot format which is :\n",
    "$$y_{one-hot} = \\begin{bmatrix}1 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 1 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 1 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 1 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 1\\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "- Let's convert y to one hot format be using **to_categorical**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd91e9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "y_valid = to_categorical(y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72442222",
   "metadata": {},
   "source": [
    "## 3- Data augmentation for training set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b148e79f",
   "metadata": {},
   "source": [
    "   - **Data augmentation** in a technique used to increase the amount of data by adding slightly modified copies of already existing data or newly created synthetic data from existing data.\n",
    "   - Do not add soo much noise so that the model underfit the data.\n",
    "   - We will use <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\">ImageDataGenerator</a> from tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9b2fd780",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = ImageDataGenerator(\n",
    "        # Rotate images by 40°\n",
    "        rotation_range = 30,\n",
    "        width_shift_range = 1,\n",
    "        height_shift_range = 1,\n",
    "        # shearing the image \n",
    "        shear_range = 1,\n",
    "        channel_shift_range = 25,\n",
    "        brightness_range = (0.95, 1.45),\n",
    "        #zoom_range = 0.1,\n",
    "        horizontal_flip = True,\n",
    "        # The area left after rotationg image will be filled with same color on image\n",
    "        fill_mode='nearest')\n",
    "\n",
    "# This is for generating cropped images\n",
    "def crop(image, padding=3):\n",
    "    p = padding\n",
    "    w, h, _ = image.shape\n",
    "    cropped_image = image[p:w-p, p:h-p]\n",
    "    cropped_image = cv2.resize(cropped_image, (IMG_SIZE, IMG_SIZE))\n",
    "    cropped_image = cropped_image.reshape(IMG_SIZE, IMG_SIZE, 1)\n",
    "    return cropped_image\n",
    "\n",
    "# This is for image blurring\n",
    "def blur(image, kernel_size=(2, 2)):\n",
    "    image_blurred = cv2.blur(image, kernel_size) \n",
    "    image_blurred = cv2.resize(image_blurred, (IMG_SIZE, IMG_SIZE))\n",
    "    image_blurred = image_blurred.reshape(IMG_SIZE, IMG_SIZE, 1)\n",
    "    return image_blurred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5b810de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display for each image the new generated image\n",
    "def show(img1, img2, img3, index):\n",
    "    w = 10\n",
    "    h = 10\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    columns = 3\n",
    "    rows = 1\n",
    "    fig.add_subplot(rows, columns, 1)\n",
    "    plt.imshow(img1, cmap=\"gray\")\n",
    "    plt.title(f'Original {index}')\n",
    "    \n",
    "    fig.add_subplot(rows, columns, 2)\n",
    "    plt.imshow(img2, cmap=\"gray\")\n",
    "    plt.title(f'Cropped {index}')\n",
    "    \n",
    "    fig.add_subplot(rows, columns, 3)\n",
    "    plt.imshow(img3, cmap=\"gray\")\n",
    "    plt.title(f'Generated {index}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae61ab04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image number 37501 out of 40000\r"
     ]
    }
   ],
   "source": [
    "# Number of generated images for each original image:\n",
    "l = 15\n",
    "\n",
    "j = 1\n",
    "index = 0\n",
    "data_aug = []\n",
    "for img in X_train:\n",
    "    img_expanded = np.expand_dims(img,0)\n",
    "    \n",
    "    # Original image\n",
    "    data_aug.append([img, y_train[index]])\n",
    "    \n",
    "    # Cropped image\n",
    "    cropped = crop(image=img, padding=3)\n",
    "    data_aug.append([cropped, y_train[index]])\n",
    "    \n",
    "    # Image blurring\n",
    "    blurred = blur(image=img, kernel_size=(2, 2))\n",
    "    data_aug.append([blurred, y_train[index]])\n",
    "    \n",
    "    # ImageDataGenerator\n",
    "    aug_iter = gen.flow(img_expanded)\n",
    "    aug_images = [next(aug_iter)[0].astype(np.uint8) for i in range(l)]\n",
    "    \n",
    "    for image in aug_images:\n",
    "        image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "        image = image.reshape((IMG_SIZE, IMG_SIZE, 1))\n",
    "        data_aug.append([image, y_train[index]])\n",
    "        # Show original image and the new created image. This is just to see is things are going well. \n",
    "        # Uncomment the following line of code if you want.\n",
    "        # show(img, cropped, image, y_train[index])\n",
    "        j += 1\n",
    "    print(f\"Image number {j} out of {(l+1)*len(X_train)}\", end='\\r')\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac1294c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the new data to X_train\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for features,label in data_aug:\n",
    "    X_train.append(features)\n",
    "    y_train.append(label)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# Normalizing for fast computation (Backpropagation)\n",
    "\n",
    "X_train = tf.keras.utils.normalize(X_train, axis=1)\n",
    "X_valid = tf.keras.utils.normalize(X_valid, axis=1)\n",
    "X_test = tf.keras.utils.normalize(X_test, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f249c530",
   "metadata": {},
   "source": [
    "# Save data in pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa1db2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I created a folder name data.pickle to save my data in\n",
    "pickle_out = open(\"data.pickle/X_train.pickle\",\"wb\")\n",
    "pickle.dump(X_train, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"data.pickle/y_train.pickle\",\"wb\")\n",
    "pickle.dump(y_train, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"data.pickle/X_valid.pickle\",\"wb\")\n",
    "pickle.dump(X_valid, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"data.pickle/y_valid.pickle\",\"wb\")\n",
    "pickle.dump(y_valid, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"data.pickle/X_test.pickle\",\"wb\")\n",
    "pickle.dump(X_test, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"data.pickle/y_test.pickle\",\"wb\")\n",
    "pickle.dump(y_test, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88848410",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e216ac49",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"data.pickle/X_train.pickle\",\"rb\")\n",
    "X_train = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"data.pickle/y_train.pickle\",\"rb\")\n",
    "y_train = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"data.pickle/X_valid.pickle\",\"rb\")\n",
    "X_valid = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"data.pickle/y_valid.pickle\",\"rb\")\n",
    "y_valid = pickle.load(pickle_in)\n",
    "\n",
    "\n",
    "pickle_in = open(\"data.pickle/X_test.pickle\",\"rb\")\n",
    "X_test = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"data.pickle/y_test.pickle\",\"rb\")\n",
    "y_test = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "048c474e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((45000, 32, 32, 1), (45000, 5))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da37ae4",
   "metadata": {},
   "source": [
    "### Build the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cd6b7c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=3, activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "model.add(Conv2D(16, kernel_size=3, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
    "#model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "# I have five classes\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69a72a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_4 (Conv2D)           (None, 30, 30, 32)        320       \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 30, 30, 32)        0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 28, 28, 16)        4624      \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 14, 14, 16)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 14, 14, 16)        0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 3136)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 15685     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,629\n",
      "Trainable params: 20,629\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff062b4",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    " - Epochs are number of passes on the entire dataset. The model will pass 5 times on the dataset. In each epoch, it will takes 20 images (batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6ddfabe4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1800/1800 [==============================] - 48s 26ms/step - loss: 0.2659 - accuracy: 0.7385 - val_loss: 0.1362 - val_accuracy: 0.9040\n",
      "Epoch 2/5\n",
      "1800/1800 [==============================] - 48s 27ms/step - loss: 0.1662 - accuracy: 0.8743 - val_loss: 0.1041 - val_accuracy: 0.9400\n",
      "Epoch 3/5\n",
      "1800/1800 [==============================] - 49s 27ms/step - loss: 0.1362 - accuracy: 0.9040 - val_loss: 0.0795 - val_accuracy: 0.9600\n",
      "Epoch 4/5\n",
      "1800/1800 [==============================] - 49s 27ms/step - loss: 0.1238 - accuracy: 0.9153 - val_loss: 0.0779 - val_accuracy: 0.9640\n",
      "Epoch 5/5\n",
      "1800/1800 [==============================] - 49s 27ms/step - loss: 0.1132 - accuracy: 0.9242 - val_loss: 0.0731 - val_accuracy: 0.9400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25ec33200d0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=25, epochs=5, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6c49a6",
   "metadata": {},
   "source": [
    "### Evaluate the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ea6434c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 5ms/step - loss: 0.0856 - accuracy: 0.9483\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.08555420488119125, 0.9483240246772766]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1d6af6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_X(threshold):\n",
    "    sample = X_test[random.randrange(0, len(X_test))]\n",
    "    image = np.expand_dims(sample, axis=0)\n",
    "    prediction = model.predict(image)\n",
    "    index = np.argmax(prediction[0])\n",
    "    probabilty = float(format(max(prediction[0]*100), \".3f\"))\n",
    "    plt.imshow(sample)\n",
    "    title = \"\"\n",
    "    if probabilty < threshold*100:\n",
    "        title = \"Unknown p<threshold\"\n",
    "    else:\n",
    "        title = f\"{CATEGORIES[index]} {probabilty}%\"\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a115ac69",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_from_X(threshold=0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928ff14c",
   "metadata": {},
   "source": [
    "### Save the model in h5 format\n",
    "\n",
    " - Always save !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e2233579",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model_h5_format/model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb9c566",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "423e5189",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"model_h5_format/model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd32ec3b",
   "metadata": {},
   "source": [
    "## Real time test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4a60374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction from image (jpg format ...):\n",
    "def predict_from_frame(image, threshold):\n",
    "    image = tf.keras.utils.normalize(image)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    prediction = model.predict(image)\n",
    "    index = np.argmax(prediction[0])\n",
    "    probabilty = float(format(max(prediction[0]*100), \".3f\"))\n",
    "    \n",
    "    if probabilty < threshold*100:\n",
    "        title = \"Unknown 'p<threshold'\"\n",
    "    else:\n",
    "        title = f\"{CATEGORIES[index]} {probabilty}%\"\n",
    "        \n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51c60b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "last_time = time()\n",
    "face_cascade = cv2.CascadeClassifier(\"cascades/data/haarcascade_frontalface_default.xml\")\n",
    "\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=2, minNeighbors=1)\n",
    "    for (x, y, w, h) in faces:\n",
    "        roi = gray[y:y+h, x:x+w]\n",
    "    \n",
    "        resized_roi = cv2.resize(roi, (IMG_SIZE, IMG_SIZE))\n",
    "    \n",
    "        color = (255, 0, 0) \n",
    "        stroke = 2\n",
    "        end_cord_x = x + w\n",
    "        end_cord_y = y + h \n",
    "        cv2.rectangle(frame, (x, y), (end_cord_x, end_cord_y), color, stroke)\n",
    "\n",
    "        p = predict_from_frame(resized_roi, threshold=0.0)   \n",
    "        cv2.putText(frame, p , (x,y), font, 1, (0, 0, 255), 1,1)\n",
    "\n",
    "\n",
    "          \n",
    "    cv2.imshow('Video', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad4d9b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "last_time = time()\n",
    "face_cascade = cv2.CascadeClassifier(\"cascades/data/haarcascade_frontalface_default.xml\")\n",
    "\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.5, minNeighbors=1)\n",
    "    if len(faces) != 0:\n",
    "        for (x, y, w, h) in faces:\n",
    "            roi = gray[y:y+h, x:x+w]\n",
    "\n",
    "            resized_roi = cv2.resize(roi, (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "            color = (255, 0, 0) \n",
    "            stroke = 2\n",
    "            end_cord_x = x + w\n",
    "            end_cord_y = y + h \n",
    "            cv2.rectangle(frame, (x, y), (end_cord_x, end_cord_y), color, stroke)\n",
    "\n",
    "            p = predict_from_frame(resized_roi, threshold=0.0)   \n",
    "            cv2.putText(frame, p , (x,y), font, 1, (0, 0, 255), 1,1)\n",
    "\n",
    "    else:\n",
    "        cv2.putText(frame, \"No face is detected\" , (x,y), font, 1, (0, 0, 255), 1,1)\n",
    "    \n",
    "    cv2.imshow('Video', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89448bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a5ebd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(faces) != 0:\n",
    "        for (x, y, w, h) in faces:\n",
    "            roi = gray[y:y+h, x:x+w]\n",
    "\n",
    "            resized_roi = cv2.resize(roi, (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "            color = (255, 0, 0) \n",
    "            stroke = 2\n",
    "            end_cord_x = x + w\n",
    "            end_cord_y = y + h \n",
    "            cv2.rectangle(frame, (x, y), (end_cord_x, end_cord_y), color, stroke)\n",
    "\n",
    "            p = predict_from_frame(resized_roi, threshold=0.0)   \n",
    "            cv2.putText(frame, p , (x,y), font, 1, (0, 0, 255), 1,1)\n",
    "\n",
    "    else:\n",
    "        cv2.putText(frame, \"No face is detected\" , (x,y), font, 1, (0, 0, 255), 1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc0c131c",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORIES = [\"aljadd\", \"nossaiba\", \"nouhaila\", \"langze\", \"unknown\", \"no person\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d7112320",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 1, 2, 4, 3, 5, 2, 4, 5, 3, 2, 3, 3, 5, 2, 4, 5, 3, 2, 3, 5, 5]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = [1, 2, 1, 2, 4, 3, 5, 2, 4, 5, 3, 2, 3, 3, 5, 2, 4, 5, 3, 2, 3, 5, 5]\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c196b399",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 4, 3, 5]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_no_duplicates = []\n",
    "for i in predictions :\n",
    "    if i not in predictions_no_duplicates:\n",
    "        predictions_no_duplicates.append(i)\n",
    "predictions_no_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7991778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 3, 1, 6]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "occurences = []\n",
    "for p in predictions_no_duplicates:\n",
    "    occurence = predictions.count(p)\n",
    "    # Do not consider duplicates of faces of known people, because it's due to face cascade errors\n",
    "    if p not in [0, 1, 2, 3]:\n",
    "        occurences.append(occurence)\n",
    "    else:\n",
    "        occurences.append(1)\n",
    "occurences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2889a209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Predicitions : [1, 2, 4, 3, 5], Occurences : [1, 1, 3, 1, 6]'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"Predicitions : {predictions_no_duplicates}, Occurences : {occurences}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e1753a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Index of \"no person is detected\"\n",
    "index_no_person = predictions_no_duplicates.index(5)\n",
    "predictions_no_duplicates.pop(index_no_person)\n",
    "occurences.pop(index_no_person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b194e49b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_no_duplicates.pop(index_no_person)\n",
    "occurences.pop(index_no_person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7d40244c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Predicitions : [1, 2, 4, 3], Occurences : [1, 1, 3, 1]'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"Predicitions : {predictions_no_duplicates}, Occurences : {occurences}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "fa07dffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_faces = len(predictions_no_duplicates)\n",
    "predictions = predictions_no_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "614b818c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Il y a  nossaiba,  nouhaila, 3 unknown,  langze, et 6 no person.'"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def replace(i):\n",
    "            if i == 1:\n",
    "                return \"\"\n",
    "            else:\n",
    "                return str(i)\n",
    "\n",
    "result = \"Il y a \"\n",
    "if number_of_faces == 1:\n",
    "    result += f\"{replace(occurences[0])}{CATEGORIES[predictions[0]]}\"\n",
    "\n",
    "elif number_of_faces == 2:\n",
    "    result += f\"{replace(occurences[0])} {CATEGORIES[predictions[0]]} et {replace(occurences[1])} {CATEGORIES[predictions[1]]}\"\n",
    "\n",
    "else:\n",
    "    i = 0\n",
    "    for p in predictions_no_duplicates:\n",
    "        if i < number_of_faces-1:\n",
    "            result += f\"{replace(occurences[i])} {CATEGORIES[p]}, \"\n",
    "        else:\n",
    "            result += f\"et {replace(occurences[i])} {CATEGORIES[p]}.\"\n",
    "        i += 1\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "50a80f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1816377",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
