{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a8e020c",
   "metadata": {},
   "source": [
    "# Multi-Face identification using CNN : Grayscale\n",
    "\n",
    "\n",
    " - In this notebook we will build our facial identification model step by step to recognize four people. Note that we have images in **jpg** format, and we will go throw the details on how to prepare the data (X, y) for our CNN model. We add an extra folder where we put images for some people that we don't know to classify them as **unknown**.\n",
    " - The model will be trained on images containing faces, what I mean by that is each image contains just the face area. We don't take the background into consideration.\n",
    " - If your images are not like that, you have two options:\n",
    "     - Manually resized them and keep just the face part.\n",
    "     - Use <a href=\"https://docs.opencv.org/3.4/db/d28/tutorial_cascade_classifier.html\">face cascade classifier</a> to extract the face part from each image and save them. If the face cascade classifier miss some faces, then you will manually resize them. \n",
    "\n",
    " - The model will run on real-time, for each video frame, we will first extract the faces using face cascade and then make predictions. For each face in an image we make prediction.\n",
    " \n",
    "First let's import some libraries :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6563aad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import random\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import load_model\n",
    "from random import randrange, uniform\n",
    "from time import time\n",
    "import glob\n",
    "import shutil\n",
    "import itertools\n",
    "from time import time\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Image size\n",
    "IMG_SIZE = 32\n",
    "\n",
    "# Dataset folder\n",
    "DATADIR = \"dataset\"\n",
    "\n",
    "# Five classes/folders\n",
    "CATEGORIES = [\"aljadd\", \"nossaiba\", \"nouhaila\", \"langze\", \"unknown\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69338739",
   "metadata": {},
   "source": [
    "# Create trainning, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "41d6d54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aljadd\n",
      "nossaiba\n",
      "nouhaila\n",
      "langze\n",
      "unknown\n"
     ]
    }
   ],
   "source": [
    "os.chdir('dataset')\n",
    "if os.path.isdir('train/aljadd') is False:\n",
    "    for category in CATEGORIES:\n",
    "        # Make train/category\n",
    "        os.makedirs(f'train/{category}')\n",
    "        \n",
    "        # Make test/category\n",
    "        os.makedirs(f'test/{category}')\n",
    "        \n",
    "        # Make valid/category\n",
    "        os.makedirs(f'valid/{category}')\n",
    "\n",
    "        \n",
    "    for category in CATEGORIES:\n",
    "        print(category)\n",
    "        # Randomly take 300 images for training\n",
    "        for c in random.sample(glob.glob(f'{category}/{category}*'), 375):\n",
    "            shutil.move(c, f'train/{category}')\n",
    "\n",
    "        # Randomly take 70 images for validation\n",
    "        for c in random.sample(glob.glob(f'{category}/{category}*'), 50):\n",
    "            shutil.move(c, f'valid/{category}')\n",
    "\n",
    "        # Move the rest to test folder\n",
    "        for img in os.listdir(f'{category}/'):\n",
    "            image = f'{category}/{img}'\n",
    "            shutil.move(image, f'test/{category}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ff7512",
   "metadata": {},
   "source": [
    "**Move back images if necessary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8edc54f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Were are still in dataset folder\n",
    "\n",
    "def reset(): # Move every images back to initial folder\n",
    "    three_folders = [\"train\", \"test\", \"valid\"]\n",
    "    for category in CATEGORIES:\n",
    "        for folder in three_folders:\n",
    "            for img in os.listdir(f'{folder}/{category}'):\n",
    "                image = fr'{folder}/{category}/{img}'\n",
    "                shutil.move(image, f'{category}')\n",
    "reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2363a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0098ddfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bc26cb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/cygdrive/c/Users/install.PO-ETU007/Desktop/MyProjects/iEars/Model training/Face identification\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bbca4e",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456dff7f",
   "metadata": {},
   "source": [
    "## 1- Create arrays of grayscale images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9dd5fb54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 375/375 [00:03<00:00, 121.58it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 375/375 [00:04<00:00, 82.77it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 375/375 [00:01<00:00, 359.49it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 375/375 [00:03<00:00, 122.28it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 375/375 [00:02<00:00, 144.74it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 143.42it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 63.32it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 350.20it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 138.15it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 145.70it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 275/275 [00:02<00:00, 128.70it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 196/196 [00:02<00:00, 76.33it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 89/89 [00:00<00:00, 376.40it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 90/90 [00:00<00:00, 154.42it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 275/275 [00:01<00:00, 152.16it/s]\n"
     ]
    }
   ],
   "source": [
    "train = []\n",
    "valid = []\n",
    "test = []\n",
    "sets = [\"train\", \"valid\", \"test\"]\n",
    "data = [train, valid, test]\n",
    "\n",
    "for s in range(0, 3):\n",
    "    \n",
    "    # Path to train, valid or test\n",
    "    path = os.path.join(\"dataset\", sets[s])\n",
    "    \n",
    "    for category in CATEGORIES:\n",
    "        \n",
    "        # index of the class: 0, ... 4\n",
    "        class_num = CATEGORIES.index(category)\n",
    "        \n",
    "        # Path to a category inside a set : eg. train/mohammed\n",
    "        full_path = os.path.join(path, category)\n",
    "        for img in tqdm(os.listdir(full_path)):\n",
    "            \n",
    "            img_path = os.path.join(full_path,img)\n",
    "            \n",
    "            # grayscale\n",
    "            img_gray = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            img_resized = cv2.resize(img_gray, (IMG_SIZE, IMG_SIZE))\n",
    "            \n",
    "            # Add data to the three sets\n",
    "            data[s].append([img_resized, class_num]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0055e3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(train)\n",
    "random.shuffle(valid)\n",
    "random.shuffle(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f6b75f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make arrays\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "X_valid = []\n",
    "y_valid = []\n",
    "\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "data = [train, valid, test]\n",
    "\n",
    "# Train\n",
    "for features,label in train:\n",
    "    X_train.append(features)\n",
    "    y_train.append(label)\n",
    "X_train = np.array(X_train).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# Valid\n",
    "for features,label in valid:\n",
    "    X_valid.append(features)\n",
    "    y_valid.append(label)\n",
    "X_valid = np.array(X_valid).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "y_valid = np.array(y_valid)\n",
    "\n",
    "# Test\n",
    "for features,label in test:\n",
    "    X_test.append(features)\n",
    "    y_test.append(label)\n",
    "X_test = np.array(X_test).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c7f611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot one image from each set\n",
    "\n",
    "# random indexs\n",
    "i1 = random.randrange(0, len(X_train))\n",
    "i2 = random.randrange(0, len(X_valid))\n",
    "i3 = random.randrange(0, len(X_test))\n",
    "\n",
    "w = 10\n",
    "h = 10\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "columns = 3\n",
    "rows = 1\n",
    "fig.add_subplot(rows, columns, 1)\n",
    "plt.imshow(X_train[i1], cmap=\"gray\")\n",
    "plt.title(f\"Train, {y_train[i1]}\")\n",
    "fig.add_subplot(rows, columns, 2)\n",
    "plt.imshow(X_valid[i2], cmap=\"gray\")\n",
    "plt.title(f\"Valid, {y_valid[i2]}\")\n",
    "fig.add_subplot(rows, columns, 3)\n",
    "plt.imshow(X_test[i3], cmap=\"gray\")\n",
    "plt.title(f\"Test, {y_test[i3]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254c48d1",
   "metadata": {},
   "source": [
    "## 2- One hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2c6727",
   "metadata": {},
   "source": [
    "- This is very important to understand. In binary classification the y outputs takes two values (0, 1) but, in our case we have five values because class_num vary from 0 to 4. \n",
    " - y is :\n",
    "$$y = \\begin{bmatrix}0\\\\\n",
    "1\\\\\n",
    "2\\\\\n",
    "3\\\\\n",
    "4\\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "- Our CNN model accept only one hot format which is :\n",
    "$$y_{one-hot} = \\begin{bmatrix}1 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 1 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 1 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 1 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 1\\\\\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "- Let's convert y to one hot format be using **to_categorical**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bd91e9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "y_valid = to_categorical(y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72442222",
   "metadata": {},
   "source": [
    "## 3- Data augmentation for training set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b148e79f",
   "metadata": {},
   "source": [
    "   - **Data augmentation** in a technique used to increase the amount of data by adding slightly modified copies of already existing data or newly created synthetic data from existing data.\n",
    "   - Do not add soo much noise so that the model underfit the data.\n",
    "   - We will use <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\">ImageDataGenerator</a> from tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9b2fd780",
   "metadata": {},
   "outputs": [],
   "source": [
    " gen = ImageDataGenerator(\n",
    "        # Rotate images by 40°\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        # shearing the image \n",
    "        shear_range=0.1,\n",
    "        channel_shift_range = 25,\n",
    "        brightness_range=(0.95, 1.45),\n",
    "        zoom_range=0.1,\n",
    "        horizontal_flip=True,\n",
    "        # The area left after rotationg image will be filled with same color on image\n",
    "        fill_mode='nearest')\n",
    "\n",
    "# This is for generating cropped images\n",
    "def crop(image, padding):\n",
    "    p = padding\n",
    "    w, h, _ = image.shape\n",
    "    cropped_image = image[p:w-p, p:h-p]\n",
    "    cropped_image = cv2.resize(cropped_image, (IMG_SIZE, IMG_SIZE))\n",
    "    cropped_image = cropped_image.reshape(IMG_SIZE, IMG_SIZE, 1)\n",
    "    return cropped_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a5b810de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display for each image the new generated image\n",
    "def show(img1, img2, img3, index):\n",
    "    w = 10\n",
    "    h = 10\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    columns = 3\n",
    "    rows = 1\n",
    "    fig.add_subplot(rows, columns, 1)\n",
    "    plt.imshow(img1, cmap=\"gray\")\n",
    "    plt.title(f'Original {index}')\n",
    "    \n",
    "    fig.add_subplot(rows, columns, 2)\n",
    "    plt.imshow(img2, cmap=\"gray\")\n",
    "    plt.title(f'Cropped {index}')\n",
    "    \n",
    "    fig.add_subplot(rows, columns, 3)\n",
    "    plt.imshow(img3, cmap=\"gray\")\n",
    "    plt.title(f'Generated {index}')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ae61ab04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image number 18751 out of 20625\r"
     ]
    }
   ],
   "source": [
    "# Number of generated images for each original image:\n",
    "l = 10\n",
    "\n",
    "j = 1\n",
    "index = 0\n",
    "data_aug = []\n",
    "for img in X_train:\n",
    "    img_expanded = np.expand_dims(img,0)\n",
    "    \n",
    "    # Original image\n",
    "    data_aug.append([img, y_train[index]])\n",
    "    \n",
    "    # Cropped image\n",
    "    cropped = crop(image=img, padding=3)\n",
    "    data_aug.append([cropped, y_train[index]])\n",
    "    \n",
    "    \n",
    "    # ImageDataGenerator\n",
    "    aug_iter = gen.flow(img_expanded)\n",
    "    aug_images = [next(aug_iter)[0].astype(np.uint8) for i in range(l)]\n",
    "    \n",
    "    for image in aug_images:\n",
    "        image = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\n",
    "        image = image.reshape((IMG_SIZE, IMG_SIZE, 1))\n",
    "        data_aug.append([image, y_train[index]])\n",
    "        # Show original image and the new created image. This is just to see is things are going well. \n",
    "        # Uncomment the following line of code if you want.\n",
    "        # show(img, cropped, image, y_train[index])\n",
    "        j += 1\n",
    "    print(f\"Image number {j} out of {(l+1)*len(X_train)}\", end='\\r')\n",
    "    index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ac1294c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the new data to X_train\n",
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "for features,label in data_aug:\n",
    "    X_train.append(features)\n",
    "    y_train.append(label)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# Normalizing for fast computation (Backpropagation)\n",
    "\n",
    "X_train = tf.keras.utils.normalize(X_train, axis=1)\n",
    "X_valid = tf.keras.utils.normalize(X_valid, axis=1)\n",
    "X_test = tf.keras.utils.normalize(X_test, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f249c530",
   "metadata": {},
   "source": [
    "# Save data in pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aa1db2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I created a folder name data.pickle to save my data in\n",
    "pickle_out = open(\"data.pickle/X_train.pickle\",\"wb\")\n",
    "pickle.dump(X_train, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"data.pickle/y_train.pickle\",\"wb\")\n",
    "pickle.dump(y_train, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"data.pickle/X_valid.pickle\",\"wb\")\n",
    "pickle.dump(X_valid, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"data.pickle/y_valid.pickle\",\"wb\")\n",
    "pickle.dump(y_valid, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"data.pickle/X_test.pickle\",\"wb\")\n",
    "pickle.dump(X_test, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"data.pickle/y_test.pickle\",\"wb\")\n",
    "pickle.dump(y_test, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88848410",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e216ac49",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_in = open(\"data.pickle/X_train.pickle\",\"rb\")\n",
    "X_train = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"data.pickle/y_train.pickle\",\"rb\")\n",
    "y_train = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"data.pickle/X_valid.pickle\",\"rb\")\n",
    "X_valid = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"data.pickle/y_valid.pickle\",\"rb\")\n",
    "y_valid = pickle.load(pickle_in)\n",
    "\n",
    "\n",
    "pickle_in = open(\"data.pickle/X_test.pickle\",\"rb\")\n",
    "X_test = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"data.pickle/y_test.pickle\",\"rb\")\n",
    "y_test = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "048c474e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((22500, 32, 32, 1), (22500, 5))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da37ae4",
   "metadata": {},
   "source": [
    "### Build the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cd6b7c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(18, kernel_size=3, activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "#model.add(Conv2D(30, kernel_size=3, activation='relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(18, kernel_size=3, activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "\n",
    "# I have five classes\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "69a72a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 30, 30, 18)        180       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 28, 28, 32)        5216      \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 26, 26, 18)        5202      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 26, 26, 18)        0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 12168)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 5)                 60845     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 71,443\n",
      "Trainable params: 71,443\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff062b4",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    " - Epochs are number of passes on the entire dataset. The model will pass 5 times on the dataset. In each epoch, it will takes 20 images (batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6ddfabe4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1125/1125 [==============================] - 38s 34ms/step - loss: 0.2987 - accuracy: 0.6871 - val_loss: 0.1389 - val_accuracy: 0.8880\n",
      "Epoch 2/5\n",
      "1125/1125 [==============================] - 41s 36ms/step - loss: 0.1897 - accuracy: 0.8418 - val_loss: 0.1025 - val_accuracy: 0.9240\n",
      "Epoch 3/5\n",
      "1125/1125 [==============================] - 45s 40ms/step - loss: 0.1478 - accuracy: 0.8877 - val_loss: 0.0818 - val_accuracy: 0.9480\n",
      "Epoch 4/5\n",
      "1125/1125 [==============================] - 45s 40ms/step - loss: 0.1138 - accuracy: 0.9214 - val_loss: 0.0638 - val_accuracy: 0.9600\n",
      "Epoch 5/5\n",
      "1125/1125 [==============================] - 38s 34ms/step - loss: 0.0956 - accuracy: 0.9360 - val_loss: 0.0504 - val_accuracy: 0.9720\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x229bf786cd0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=20, epochs=5, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6c49a6",
   "metadata": {},
   "source": [
    "### Evaluate the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6ea6434c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29/29 [==============================] - 0s 10ms/step - loss: 0.0540 - accuracy: 0.9773\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.0540069155395031, 0.977297306060791]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b1d6af6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_from_X(threshold):\n",
    "    sample = X_test[random.randrange(0, len(X_test))]\n",
    "    image = np.expand_dims(sample, axis=0)\n",
    "    prediction = model.predict(image)\n",
    "    index = np.argmax(prediction[0])\n",
    "    probabilty = float(format(max(prediction[0]*100), \".3f\"))\n",
    "    plt.imshow(sample)\n",
    "    title = \"\"\n",
    "    if probabilty < threshold*100:\n",
    "        title = \"Unknown p<threshold\"\n",
    "    else:\n",
    "        title = f\"{CATEGORIES[index]} {probabilty}%\"\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a115ac69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict_from_X(0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928ff14c",
   "metadata": {},
   "source": [
    "### Save the model in h5 format\n",
    "\n",
    " - Always save !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e2233579",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model_h5_format/model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb9c566",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "423e5189",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"model_h5_format/model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd32ec3b",
   "metadata": {},
   "source": [
    "## Real time test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e4a60374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction from image (jpg format ...):\n",
    "def predict_from_frame(image, threshold):\n",
    "    image = tf.keras.utils.normalize(image)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    prediction = model.predict(image)\n",
    "    index = np.argmax(prediction[0])\n",
    "    probabilty = float(format(max(prediction[0]*100), \".3f\"))\n",
    "    \n",
    "    if probabilty < threshold*100:\n",
    "        title = \"Unknown 'p<threshold'\"\n",
    "    else:\n",
    "        title = f\"{CATEGORIES[index]} {probabilty}%\"\n",
    "        \n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "51c60b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "last_time = time()\n",
    "face_cascade = cv2.CascadeClassifier(\"cascades/data/haarcascade_frontalface_default.xml\")\n",
    "\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=2, minNeighbors=1)\n",
    "    for (x, y, w, h) in faces:\n",
    "        roi = gray[y:y+h, x:x+w]\n",
    "    \n",
    "        resized_roi = cv2.resize(roi, (IMG_SIZE, IMG_SIZE))\n",
    "    \n",
    "        color = (255, 0, 0) \n",
    "        stroke = 2\n",
    "        end_cord_x = x + w\n",
    "        end_cord_y = y + h \n",
    "        cv2.rectangle(frame, (x, y), (end_cord_x, end_cord_y), color, stroke)\n",
    "\n",
    "        p = predict_from_frame(resized_roi, threshold=0.4)   \n",
    "        cv2.putText(frame, p , (x,y), font, 1, (0, 0, 255), 1,1)\n",
    "\n",
    "\n",
    "          \n",
    "    cv2.imshow('Video', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214940b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
