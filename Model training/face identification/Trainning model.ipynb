{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a8e020c",
   "metadata": {},
   "source": [
    "# Multi-Face identification using CNN\n",
    "\n",
    "\n",
    " - In this notebook we will build our facial identification model step by step to recognize four people. Note that we have images in **jpg** format, and we will go throw the details on how to prepare the data (X, y) for our CNN model. We add folder where we put images for people that we don't know to classify them as **unknown**.\n",
    " - The model will be trained on images containing faces, what I mean by that is each image contains just the face area. We don't take the background in consideration.\n",
    " - What if your images are not like that, well you have to options:\n",
    "     - Manually resized them.\n",
    "     - Use face cascade classifier to extract the face part from each image and save them. Watch this video and stop at the 16th minute : https://www.youtube.com/watch?v=PmZ29Vta7Vc&ab_channel=CodingEntrepreneurs. This classifier may miss some faces, and what we did is manually resize the missed images. \n",
    "\n",
    " - You may ask yourself why all this. The model we run on real-time, for each video frame, we will first extract the faces using face cascade, say 5 faces, and run prediction on those faces and show all the 5 prediction.\n",
    " \n",
    "First let's import some libraries :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6563aad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "import pickle\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import random\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import load_model\n",
    "from random import randrange, uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf3ecc2",
   "metadata": {},
   "source": [
    "    \n",
    "   - We will set the image shape to 64x64.\n",
    "   - DATACROP : directory where we have four folders, each one contains images for each person.\n",
    "   - CATEGORIES : name of the four folders(people).\n",
    "   - One example: **\"../DATACROP/AL JADD/myimage.jpg\"**.\n",
    "   - Look where I put the directory next to the jupyter notebook:\n",
    "<img src=\"dire.png\">\n",
    "<img src=\"5classes.png\"  width=\"500\" height=\"600\">\n",
    "   - I put also the cascades folder used for face cascade on video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ac55992",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 32\n",
    "DATACROP = \"dataset_crop\"\n",
    "CATEGORIES = [\"AL JADD\", \"Nossaiba\", \"EL NABAOUI\", \"YE\", \"unknown\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52328fd5",
   "metadata": {},
   "source": [
    "### Data augmentation\n",
    "\n",
    "   - **Data augmentation** in a technique used to increase the amount of data by adding slightly modified copies of already existing data or newly created synthetic data from existing data.\n",
    "   - we will rotate images by **10°** because our model will process images taken by a camera and in our contest, we know if a person stands in front of the camera, his face will not be rotated by a big angle. How you will augmente your data depends on your usecase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9152c9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = ImageDataGenerator(\n",
    "        # Rotate images by 10°\n",
    "        rotation_range=10,\n",
    "        shear_range=0.01,\n",
    "        channel_shift_range = 5,\n",
    "        brightness_range=(0.1, 1),\n",
    "        horizontal_flip=True,\n",
    "        # The area left after rotationg image will be filled with same color on image\n",
    "        fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061b6e8d",
   "metadata": {},
   "source": [
    "### Prepare data for keras\n",
    "\n",
    " - Now will load images, apply data augmentation and convert them to numpy arrays.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a13b6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 187/187 [00:57<00:00,  3.27it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:12<00:00,  8.02it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 92/92 [00:18<00:00,  4.91it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 134/134 [00:42<00:00,  3.13it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 208/208 [03:43<00:00,  1.07s/it]\n"
     ]
    }
   ],
   "source": [
    "# data : array contains X,y\n",
    "data = []\n",
    "for category in CATEGORIES:\n",
    "    # path for \"DATACROP/category\" : Example : \"dataset_crop/AL JADD/\". dataset_crop is our directory.\n",
    "    path = os.path.join(DATACROP,category) \n",
    "    # class_num is the index of category in CATEGORIES, so AL JADD has the index 0\n",
    "    class_num = CATEGORIES.index(category) \n",
    "    for img in tqdm(os.listdir(path)):\n",
    "        # Image path with its name\n",
    "        img_path = os.path.join(path,img)\n",
    "        \n",
    "        # load image with cv2\n",
    "        img_colored = cv2.imread(img_path)\n",
    "        \n",
    "        # The generator takes batches, the same of a batch is (n, r, g, b) where n is the number of images in the batch,\n",
    "        #r, g and b are the dimention of each image on the batch.\n",
    "        # img_colored is one image and by applying the following line of code it will have the shape (1, r, g, b).\n",
    "        img_expanded = np.expand_dims(img_colored,0)\n",
    "        \n",
    "        # This line of code is essential\n",
    "        aug_iter = gen.flow(img_expanded)\n",
    "        \n",
    "        # For each image we will generate 10 images\n",
    "        aug_images = [next(aug_iter)[0].astype(np.uint8) for i in range(10)]\n",
    "        for image in aug_images:\n",
    "            \n",
    "             # For each generated image, we will convert it to grayscale and resize it so that the model training will be very\n",
    "             # fast. Now every image is an array.\n",
    "            img_gray_aug = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            img_gray_aug_resized = cv2.resize(img_gray_aug, (IMG_SIZE, IMG_SIZE))\n",
    "            \n",
    "            # add the image array and the index. The 10 images will have the same class_num because they are generated from\n",
    "            # the same image. \n",
    "            # Example : image1.jpg of \"AL JADD\" has index 0 because \"AL JADD\" is the first in CATEGORIES. So the generated \n",
    "            # images 1image1.jpg, ..., 10image1.jpg will have the index 0. Those image names are just examples.\n",
    "            data.append([img_gray_aug_resized, class_num])        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e3a80d",
   "metadata": {},
   "source": [
    "### Shuffle the data\n",
    "\n",
    "  - Remember to shuffle the data so that the model gets diffrent classes in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b2a8823",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2c6727",
   "metadata": {},
   "source": [
    "### One hot for outputs\n",
    "\n",
    " - This is very important to understand. In binary classification the y outputs takes two values (0, 1) but, in our case we have five values because class_num vary from 0 to 4. So we will convert y to one hot.\n",
    " - y is :\n",
    "$$y = \\begin{bmatrix}0\\\\\n",
    "1\\\\\n",
    "2\\\\\n",
    "3\\\\\n",
    "4\\\\\n",
    "\\end{bmatrix}$$\n",
    "- y after one hot transformation :\n",
    "$$y_{one-hot} = \\begin{bmatrix}1 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 1 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 1 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 1 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 1\\\\\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a5a7859",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "\n",
    "for features,label in data:\n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "\n",
    "# One hot :\n",
    "y = to_categorical(y)\n",
    "\n",
    "# Images are on grayscale (a, b) (X shape is (m, a, b) where m number of images) shape. \n",
    "# We want (a, b, 1) (X (m, a, b, 1) shape :\n",
    "X = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 1)\n",
    "y = np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f2aafe9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7210, 32, 32, 1)\n",
      "(7210, 5)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ac7009",
   "metadata": {},
   "source": [
    "### Save data as pickle format\n",
    "\n",
    " - Always save your data so next time you will just load X and y without repeating the above codes again "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4a0ec3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_out = open(\"X.pickle\",\"wb\")\n",
    "pickle.dump(X, pickle_out)\n",
    "pickle_out.close()\n",
    "\n",
    "pickle_out = open(\"y.pickle\",\"wb\")\n",
    "pickle.dump(y, pickle_out)\n",
    "pickle_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199ea3c0",
   "metadata": {},
   "source": [
    "<img src=\"dire2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe52081c",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0fc28748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One training example shape: (32, 32, 1)\n",
      "Output shape: (5,)\n"
     ]
    }
   ],
   "source": [
    "pickle_in = open(\"X.pickle\",\"rb\")\n",
    "X = pickle.load(pickle_in)\n",
    "\n",
    "pickle_in = open(\"y.pickle\",\"rb\")\n",
    "y = pickle.load(pickle_in)\n",
    "\n",
    "# Rescale the image. The values will vary from 0 to 1 for fast training\n",
    "X = X/255.0\n",
    "print(f'One training example shape: {X.shape[1:]}')\n",
    "print(f'Output shape: {y.shape[1:]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7166f65c",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "27c32197",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da37ae4",
   "metadata": {},
   "source": [
    "### Build the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd6b7c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# The input shape is the image shape (a, b, 1) 'It is grayscale'\n",
    "model.add(Conv2D(32, kernel_size=3, activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1)))\n",
    "model.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
    "model.add(Conv2D(128, kernel_size=3, activation='relu'))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Conv2D(128, kernel_size=3, activation='relu'))\n",
    "model.add(Conv2D(64, kernel_size=3, activation='relu'))\n",
    "model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
    "#model.add(Dropout(0.6))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "# I have four classes, so make sure to use the right number bellow\n",
    "model.add(Dense(5, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff062b4",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    " - Epochs are number of passes on the entire dataset. The model will pass 5 times on the dataset. In each epoch, it will takes 20 images (batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6ddfabe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "130/130 [==============================] - 28s 208ms/step - loss: 0.3701 - accuracy: 0.5511 - val_loss: 0.1178 - val_accuracy: 0.9295\n",
      "Epoch 2/5\n",
      "130/130 [==============================] - 28s 213ms/step - loss: 0.1242 - accuracy: 0.9214 - val_loss: 0.0882 - val_accuracy: 0.9451\n",
      "Epoch 3/5\n",
      "130/130 [==============================] - 28s 215ms/step - loss: 0.0645 - accuracy: 0.9595 - val_loss: 0.0455 - val_accuracy: 0.9752\n",
      "Epoch 4/5\n",
      "130/130 [==============================] - 28s 215ms/step - loss: 0.0401 - accuracy: 0.9788 - val_loss: 0.0399 - val_accuracy: 0.9798\n",
      "Epoch 5/5\n",
      "130/130 [==============================] - 28s 217ms/step - loss: 0.0352 - accuracy: 0.9842 - val_loss: 0.0262 - val_accuracy: 0.9896\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14ff2c97f70>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=20, epochs=5, validation_split=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a123159",
   "metadata": {},
   "source": [
    "### Evaluate the model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2db6b41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91/91 [==============================] - 5s 51ms/step - loss: 0.0371 - accuracy: 0.9806\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.03709942102432251, 0.9805825352668762]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)\n",
    "\n",
    "# If the model suffer from overfitting, try to change CNN architecture by adding Droput layers ... \n",
    "# You may spend a lot time retraining your model. DO NOT BE SAD! ... You have to know what your model suffer from, and try to \n",
    "# solve the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928ff14c",
   "metadata": {},
   "source": [
    "### Save the model in h5 format\n",
    "\n",
    " - Always save !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e2233579",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb9c566",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "423e5189",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_5 (Conv2D)           (None, 30, 30, 32)        320       \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 28, 28, 64)        18496     \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 26, 26, 128)       73856     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 26, 26, 128)       0         \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 24, 24, 128)       147584    \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 22, 22, 64)        73792     \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 20, 20, 32)        18464     \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 12800)             0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 64005     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 396,517\n",
      "Trainable params: 396,517\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model = load_model(\"my_model.h5\")\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5271fac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction from X_test:\n",
    "# I will set threshold = 0.5 so if the model gives for example 0.4 probability that an image belongs to a class, I will consider\n",
    "# not taking it into consideration\n",
    "def predict_from_X(i, threshold):\n",
    "    x = X_test[i]\n",
    "    plt.imshow(x)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    prediction = new_model.predict(x)\n",
    "    i  =  np.array(np.where(((prediction >= threshold).astype('int32')[0]) == 1))[0]\n",
    "    who = \"\"\n",
    "    \n",
    "    if i.shape == (0,):\n",
    "        who = \"Person not recognized\"\n",
    "    else:\n",
    "        \n",
    "        if i[0] == 0:\n",
    "            who = \"AL JADD\"\n",
    "\n",
    "        elif i[0] == 1:\n",
    "            who = \"Nossaiba\"\n",
    "\n",
    "        elif i[0] == 2:\n",
    "            who = \"EL NABAOUI\"\n",
    "\n",
    "        elif i[0] == 3:\n",
    "            who = \"YE Langze\"\n",
    "        else:\n",
    "            pass\n",
    "    print(who)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a68bee34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction from X_test:\n",
    "# I will set threshold = 0.5 so if the model gives for example 0.4 probability that an image belongs to a class, I will consider\n",
    "# not taking it into consideration\n",
    "def predict_from_X(i, threshold):\n",
    "    x = X_test[i]\n",
    "    plt.imshow(x)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    prediction = new_model.predict(x)\n",
    "    i  =  np.array(np.where(((prediction >= threshold).astype('int32')[0]) == 1))[0]\n",
    "    who = \"\"\n",
    "    \n",
    "    if not i.shape == (0,):\n",
    "        if i[0] == 0:\n",
    "                who = \"AL JADD\"\n",
    "\n",
    "        elif i[0] == 1:\n",
    "            who = \"Nossaiba\"\n",
    "\n",
    "        elif i[0] == 2:\n",
    "            who = \"EL NABAOUI\"\n",
    "\n",
    "        elif i[0] == 3:\n",
    "            who = \"YE Langze\"\n",
    "        else:\n",
    "            who = \"Unknown\"\n",
    "    else:\n",
    "        who = \"Unknown\"\n",
    "    print(who)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9f1b7cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unknown\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict_from_X(randrange(0, X_test.shape[0]-1), threshold=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd32ec3b",
   "metadata": {},
   "source": [
    "## Real time test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e4a60374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction from image (jpg format ...):\n",
    "def predict_from_frame(image, threshold):\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    prediction = new_model.predict(image)\n",
    "    i  =  np.array(np.where(((prediction >= threshold).astype('int32')[0]) == 1))[0]\n",
    "    who = \"\"\n",
    "    if not i.shape == (0,):\n",
    "        if i[0] == 0:\n",
    "                who = \"AL JADD\"\n",
    "\n",
    "        elif i[0] == 1:\n",
    "            who = \"Nossaiba\"\n",
    "\n",
    "        elif i[0] == 2:\n",
    "            who = \"EL NABAOUI\"\n",
    "\n",
    "        elif i[0] == 3:\n",
    "            who = \"YE Langze\"\n",
    "        else:\n",
    "            who = \"Unknown\"\n",
    "    else:\n",
    "        who = \"Unknown\"\n",
    "    return who"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "51c60b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "cap = cv2.VideoCapture(0)\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "last_time = time()\n",
    "face_cascade = cv2.CascadeClassifier(\"cascades/data/haarcascade_frontalface_default.xml\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=2, minNeighbors=1)\n",
    "    for (x, y, w, h) in faces:\n",
    "            roi_gray = gray[y:y+h, x:x+w]\n",
    "            roi_sav = frame[y:y+h, x:x+w]\n",
    "            resized_roi_gray = cv2.resize(roi_gray, (IMG_SIZE, IMG_SIZE))\n",
    "            # When you wanna make prediction, you have to rescale the image first. I pass two days figuring out why my model\n",
    "            # outputing the same result. As the model was trained on rescaled images, the prediction must done on rescaled\n",
    "            # images too.\n",
    "            resized_roi_gray_recaled = resized_roi_gray/255.0\n",
    "            \n",
    "            color = (255, 0, 0) \n",
    "            stroke = 2\n",
    "            end_cord_x = x + w\n",
    "            end_cord_y = y + h \n",
    "            cv2.rectangle(frame, (x, y), (end_cord_x, end_cord_y), color, stroke)\n",
    "            \n",
    "            p = predict_from_frame(resized_roi_gray_recaled, threshold=0.8)   \n",
    "            cv2.putText(frame, p , (x,y), font, 1, (0, 0, 255), 1, cv2.LINE_4)\n",
    "          \n",
    "    cv2.imshow('Video', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "    \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bff3696",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
